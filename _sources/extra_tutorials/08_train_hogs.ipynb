{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training HOG-based AU detectors\n",
    "*written by Tiankang Xie*  \n",
    "\n",
    "In the tutorial we will demonstrate how to train the HOG-based AU models as described in our paper. \n",
    "The tutorial is split into 3 parts, where the first part demonstrates how to extract hog features from the dataset,\n",
    "and the second part demonstrates how to use the extracted hogs to perform statistical learning, the third part will be\n",
    "to demonstrate how to test the trained models with additional test data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Extracting HOGs and Landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To speed up training the HOGs, we will first try to extract the HOG and landmark features from image paths using py-feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from feat.utils import set_torch_device\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from skimage import draw\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from feat.utils.image_operations import extract_face_from_landmarks\n",
    "\n",
    "from PIL import Image\n",
    "from itertools import product\n",
    "import os \n",
    "from torchvision.transforms import Compose, Normalize, Grayscale\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from feat import Detector\n",
    "\n",
    "from joblib import delayed, Parallel\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.io import read_image, read_video\n",
    "from torch.utils.data import Dataset\n",
    "from feat.transforms import Rescale\n",
    "import glob\n",
    "from skimage.feature import hog\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from feat.data import (\n",
    "    Fex,\n",
    "    ImageDataset,\n",
    "    VideoDataset,\n",
    "    _inverse_face_transform,\n",
    "    _inverse_landmark_transform,\n",
    ")\n",
    "import glob\n",
    "from feat.utils.image_operations import (\n",
    "    extract_face_from_landmarks,\n",
    "    extract_face_from_bbox,\n",
    "    convert_image_to_tensor,\n",
    "    BBox,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_df = pd.read_csv('/home/tiankang/AU_Dataset/EmotioNet/EmotioNet_master.csv', index_col=0)\n",
    "# This is the file of the AU annotations.\n",
    "# It should look like something: |filepath|AU1|AU2|AU3..., where the first filepath column indicates the filepath of the input image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "      <th>AU1</th>\n",
       "      <th>AU2</th>\n",
       "      <th>AU4</th>\n",
       "      <th>AU5</th>\n",
       "      <th>AU6</th>\n",
       "      <th>AU9</th>\n",
       "      <th>AU12</th>\n",
       "      <th>AU17</th>\n",
       "      <th>AU20</th>\n",
       "      <th>AU25</th>\n",
       "      <th>AU26</th>\n",
       "      <th>AU43</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Storage/Data/EmotioNet/imgs/N_0000000001_0000...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Storage/Data/EmotioNet/imgs/N_0000000001_0000...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Storage/Data/EmotioNet/imgs/N_0000000001_0000...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Storage/Data/EmotioNet/imgs/N_0000000001_0000...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Storage/Data/EmotioNet/imgs/N_0000000001_0000...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filepath  AU1    AU2  AU4  AU5  \\\n",
       "0  /Storage/Data/EmotioNet/imgs/N_0000000001_0000...  0.0  999.0  1.0  0.0   \n",
       "1  /Storage/Data/EmotioNet/imgs/N_0000000001_0000...  0.0    0.0  0.0  0.0   \n",
       "2  /Storage/Data/EmotioNet/imgs/N_0000000001_0000...  1.0    0.0  1.0  1.0   \n",
       "3  /Storage/Data/EmotioNet/imgs/N_0000000001_0000...  0.0    0.0  0.0  0.0   \n",
       "4  /Storage/Data/EmotioNet/imgs/N_0000000001_0000...  0.0    0.0  1.0  0.0   \n",
       "\n",
       "   AU6  AU9  AU12  AU17  AU20  AU25  AU26  AU43  \n",
       "0  0.0  0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "1  0.0  0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "2  0.0  0.0   1.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3  0.0  0.0   0.0   0.0   0.0   0.0   1.0   0.0  \n",
       "4  0.0  0.0   0.0   0.0   0.0   1.0   0.0   0.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "au_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 218/21938 [00:12<19:32, 18.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 302/21938 [00:16<20:07, 17.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 337/21938 [00:18<19:27, 18.50it/s]WARNING:root:Warning: NO FACE is detected\n",
      "  3%|▎         | 605/21938 [00:33<20:56, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 987/21938 [00:53<21:58, 15.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1208/21938 [01:04<19:44, 17.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1258/21938 [01:07<18:34, 18.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 1372/21938 [01:13<21:06, 16.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1585/21938 [01:24<17:33, 19.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2066/21938 [01:49<17:49, 18.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2318/21938 [02:02<17:28, 18.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 2436/21938 [02:09<17:02, 19.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2578/21938 [02:16<16:13, 19.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2704/21938 [02:23<19:09, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3161/21938 [02:46<16:18, 19.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 4534/21938 [03:57<15:30, 18.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 4689/21938 [04:05<14:58, 19.20it/s]WARNING:root:Warning: NO FACE is detected\n",
      " 22%|██▏       | 4865/21938 [04:14<15:39, 18.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5013/21938 [04:22<16:13, 17.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 5401/21938 [04:42<15:46, 17.47it/s]WARNING:root:Warning: NO FACE is detected\n",
      " 25%|██▍       | 5483/21938 [04:47<14:43, 18.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5573/21938 [04:51<15:09, 17.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 5757/21938 [05:01<14:06, 19.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 6191/21938 [05:23<14:05, 18.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6292/21938 [05:28<14:03, 18.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 6940/21938 [06:02<12:33, 19.90it/s]WARNING:root:Warning: NO FACE is detected\n",
      " 32%|███▏      | 6984/21938 [06:04<13:07, 18.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7106/21938 [06:11<14:20, 17.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 7463/21938 [06:29<12:13, 19.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 7577/21938 [06:35<12:26, 19.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 8151/21938 [07:04<12:09, 18.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 8678/21938 [07:32<13:01, 16.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8796/21938 [07:38<11:36, 18.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 9148/21938 [07:56<12:48, 16.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 9569/21938 [08:17<11:14, 18.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 9841/21938 [08:31<10:32, 19.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9926/21938 [08:35<10:13, 19.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 9984/21938 [08:39<10:56, 18.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10987/21938 [09:30<10:08, 17.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11455/21938 [09:54<09:31, 18.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 11777/21938 [10:10<08:56, 18.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 11811/21938 [10:12<08:59, 18.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 12008/21938 [10:22<08:22, 19.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 12969/21938 [11:11<08:02, 18.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 13008/21938 [11:13<08:00, 18.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 13136/21938 [11:19<07:25, 19.78it/s]WARNING:root:Warning: NO FACE is detected\n",
      " 63%|██████▎   | 13714/21938 [11:49<07:22, 18.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 13855/21938 [11:56<06:54, 19.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 13862/21938 [11:56<07:40, 17.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 13959/21938 [12:01<06:40, 19.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 14161/21938 [12:11<07:00, 18.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 14359/21938 [12:22<06:51, 18.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 14458/21938 [12:27<06:47, 18.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 14776/21938 [12:43<06:12, 19.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 14821/21938 [12:46<06:07, 19.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 14848/21938 [12:47<06:08, 19.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 14871/21938 [12:48<06:10, 19.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 15191/21938 [13:05<06:30, 17.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 15623/21938 [13:27<05:38, 18.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 15834/21938 [13:38<05:25, 18.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 16542/21938 [14:15<04:42, 19.11it/s]WARNING:root:Warning: NO FACE is detected\n",
      " 77%|███████▋  | 16801/21938 [14:29<04:26, 19.28it/s]WARNING:root:Warning: NO FACE is detected\n",
      " 77%|███████▋  | 16834/21938 [14:30<04:14, 20.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 16858/21938 [14:31<04:18, 19.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 17269/21938 [14:53<04:12, 18.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 17451/21938 [15:03<04:16, 17.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 18663/21938 [16:06<02:40, 20.40it/s]WARNING:root:Warning: NO FACE is detected\n",
      " 85%|████████▌ | 18739/21938 [16:10<02:54, 18.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 18940/21938 [16:20<02:41, 18.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 19045/21938 [16:26<02:41, 17.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 19089/21938 [16:28<02:32, 18.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 19816/21938 [17:06<02:01, 17.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 19957/21938 [17:13<01:42, 19.40it/s]WARNING:root:Warning: NO FACE is detected\n",
      " 95%|█████████▍| 20830/21938 [17:59<00:57, 19.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20904/21938 [18:02<00:54, 19.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 21318/21938 [18:24<00:31, 19.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "something went wrong with reading the image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21938/21938 [18:56<00:00, 19.31it/s]\n"
     ]
    }
   ],
   "source": [
    "detector = Detector(face_model='retinaface',emotion_model='resmasknet', landmark_model=\"mobilefacenet\", au_model='svm')\n",
    "\n",
    "SAVE_HOG_DIR = '/Storage/Projects/pyfeat_testing/HOGFeatures/MyHOGFeatures/'\n",
    "input_file_list = au_df['filepath'].to_list()\n",
    "\n",
    "if not os.path.exists(SAVE_HOG_DIR):\n",
    "    os.makedirs(SAVE_HOG_DIR)\n",
    "\n",
    "# a list of all path figures\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    ImageDataset(\n",
    "        input_file_list,\n",
    "        output_size=256,\n",
    "        preserve_aspect_ratio=True,\n",
    "        padding=True,\n",
    "    ),\n",
    "    num_workers=0,\n",
    "    batch_size=1,\n",
    "    pin_memory=False,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "def _batch_hog(frames, landmarks):\n",
    "    \"\"\"\n",
    "    Helper function used in batch processing hog features\n",
    "\n",
    "    Args:\n",
    "        frames: a batch of frames\n",
    "        landmarks: a list of list of detected landmarks\n",
    "\n",
    "    Returns:\n",
    "        hog_features: a numpy array of hog features for each detected landmark\n",
    "        landmarks: updated landmarks\n",
    "    \"\"\"\n",
    "    frames = convert_image_to_tensor(frames, img_type=\"float32\")\n",
    "\n",
    "    hog_features = []\n",
    "    hog_images = []\n",
    "    new_landmark_frames = []\n",
    "    for i, frame_landmark in enumerate(landmarks):\n",
    "        if len(frame_landmark) != 0:\n",
    "            new_landmarks_faces = []\n",
    "            for j in range(len(frame_landmark)):\n",
    "                convex_hull, new_landmark = extract_face_from_landmarks(\n",
    "                    frame=frames[i],\n",
    "                    landmarks=frame_landmark[j],\n",
    "                    face_size=112,\n",
    "                )\n",
    "                fd, hog_image=hog(\n",
    "                        transforms.ToPILImage()(convex_hull[0] / 255.0),\n",
    "                        orientations=8,\n",
    "                        pixels_per_cell=(8, 8),\n",
    "                        cells_per_block=(2, 2),\n",
    "                        visualize=True,\n",
    "                        channel_axis=-1,\n",
    "                    )\n",
    "                                \n",
    "                hog_features.append(fd)\n",
    "                hog_images.append(hog_image)\n",
    "                new_landmarks_faces.append(new_landmark)\n",
    "            \n",
    "            new_landmark_frames.append(new_landmarks_faces)\n",
    "        else:\n",
    "            hog_features.append(\n",
    "                np.zeros((1, 5408))\n",
    "            )  # LC: Need to confirm this size is fixed.\n",
    "            new_landmark_frames.append([np.zeros((68, 2))])\n",
    "\n",
    "    return (hog_features, hog_images, new_landmark_frames)\n",
    "\n",
    "\n",
    "for cc, batch_data in enumerate(tqdm(data_loader)):\n",
    "    # Iterate through all the images in dataloader to get the hog feature and landmark feature\n",
    "    try:\n",
    "        faces = detector.detect_faces(\n",
    "                batch_data[\"Image\"],\n",
    "                threshold=0.5)\n",
    "                \n",
    "        landmarks = detector.detect_landmarks(\n",
    "            batch_data[\"Image\"],\n",
    "            detected_faces=faces)\n",
    "\n",
    "        hog_features, hog_images, new_landmark_frames = _batch_hog(batch_data[\"Image\"], landmarks)\n",
    "\n",
    "        for i in range(len(hog_features)):\n",
    "            with open(SAVE_HOG_DIR+batch_data['FileNames'][i].split('/')[-1].split('.')[0]+'.pkl', 'wb') as fp:\n",
    "                pickle.dump((hog_features[i], new_landmark_frames[i][0]), fp)\n",
    "\n",
    "    except:\n",
    "        print('something went wrong with reading the image')\n",
    "        continue;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Conduct Dimension Reduction on HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_dataset(saved_hog_path, au_df):\n",
    "    \"\"\"compile the saved hog and landmark features \n",
    "    Args:\n",
    "        saved_hog_path: where you saved the HOGs in the last section\n",
    "        au_df: a pandas dataframe that contains filepaths and AU annotations\n",
    "\n",
    "    Returns:\n",
    "        np.stack(hog_feats): a numpy array of hog features \n",
    "        np.stack(land_feats): a numpy array of landmarks \n",
    "        au_df.iloc[all_valid, :]: a pandas df of AU annotations \n",
    "    \"\"\"\n",
    "        \n",
    "    all_valid = [] # Which images are valid images detectable by Py-Feat?\n",
    "    hog_feats = [] # Aggregated HOG Features\n",
    "    land_feats = [] # Aggregated Landmark Features\n",
    "    all_o_filename = au_df['filepath'].to_list() # Filenames in the annotation file\n",
    "    all_hog_fp = [saved_hog_path+os.path.basename(op).split('.')[0]+'.pkl' for op in all_o_filename]\n",
    "\n",
    "    for ji in range(len(all_hog_fp)):\n",
    "        with open(all_hog_fp[ji], 'rb') as fp:\n",
    "            hog_feat, new_lands = pickle.load(fp)\n",
    "        if (len(hog_feat) == 5408) and (new_lands.shape[0] == 68) and (new_lands.shape[1] == 2): # Restrict to valid HOGs\n",
    "            all_valid.append(True)\n",
    "            hog_feats.append(hog_feat)\n",
    "            land_feats.append(new_lands)\n",
    "        else:\n",
    "            all_valid.append(False)\n",
    "\n",
    "    return np.stack(hog_feats), np.stack(land_feats), au_df.iloc[all_valid, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_hogs, trained_land, labels_df = compile_dataset(saved_hog_path='/Storage/Projects/pyfeat_testing/HOGFeatures/MyHOGFeatures/',\n",
    "                                                        au_df=au_df)\n",
    "trained_land = trained_land.reshape(trained_hogs.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21929, 5408)\n",
      "(21929, 136)\n",
      "(21929, 13)\n"
     ]
    }
   ],
   "source": [
    "print(trained_hogs.shape)\n",
    "print(trained_land.shape)\n",
    "print(labels_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that it is possible to use only upper face features / only lower face features / full face features to predict AUs\n",
    "# For demonstration purporses we will only be using full face features in the later section.\n",
    "scaler_upper = StandardScaler()\n",
    "scaler_lower = StandardScaler()\n",
    "scaler_full = StandardScaler()\n",
    "\n",
    "pca_full = PCA(n_components=0.95)\n",
    "pca_upper = PCA(n_components=0.98)\n",
    "pca_lower = PCA(n_components=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_data_upper = trained_hogs.copy()\n",
    "hog_data_upper[:, 2414:] = 0 # Restrict to upper feature \n",
    "hog_data_upper_std = scaler_upper.fit_transform(hog_data_upper)\n",
    "hog_data_upper_transformed = pca_upper.fit_transform(hog_data_upper_std)\n",
    "del hog_data_upper, hog_data_upper_std\n",
    "\n",
    "hog_data_lower = trained_hogs.copy()\n",
    "hog_data_lower[:, 0:2221] = 0 # Restrict to lower feature \n",
    "hog_data_lower_std = scaler_lower.fit_transform(hog_data_lower)\n",
    "hog_data_lower_transformed = pca_lower.fit_transform(hog_data_lower_std)\n",
    "del hog_data_lower, hog_data_lower_std\n",
    "\n",
    "hog_data_full_std = scaler_full.fit_transform(trained_hogs)\n",
    "hog_data_full_transformed = pca_full.fit_transform(hog_data_full_std)\n",
    "del hog_data_full_std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Prepare training data & label, and conduct Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from tqdm import tqdm \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutorial we will be using full face features as an example. Feel free to use upper / lower\n",
    "x_features = np.concatenate([hog_data_full_transformed, trained_land], 1)\n",
    "# In this tutorial we will be using only AU1\n",
    "y_features = labels_df['AU1'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_samp(x, y):\n",
    "    return x, y\n",
    "\n",
    "def _run_and_testData(clf, x_features, y_features, sampling_method='under', cv_n_splits=5):\n",
    "    \"\"\"\n",
    "    This function runs n-fold cross-validation on the dataset\n",
    "    \"\"\"\n",
    "    if sampling_method == 'under':\n",
    "        ros = RandomUnderSampler(random_state=0)\n",
    "    elif sampling_method == 'smote':\n",
    "        ros1 = SMOTE(random_state=0, sampling_strategy=0.60)\n",
    "        ros2 = RandomUnderSampler(random_state=0, sampling_strategy=0.5)\n",
    "        ros = Pipeline(steps=[('o', ros1), ('u', ros2)])\n",
    "    else:\n",
    "        ros = FunctionSampler(func=func_samp)\n",
    "\n",
    "    valid_train_idx = np.where(np.logical_or(y_features == 0, y_features==1))[0]\n",
    "    x_training_valid, y_training_valid = x_features[valid_train_idx, :], y_features[valid_train_idx]\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=cv_n_splits, random_state=1, shuffle=True)\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(X=x_training_valid, y=y_training_valid)):\n",
    "    \n",
    "        xx_train, xx_val = x_training_valid[train_index], x_training_valid[val_index]\n",
    "        yy_train, yy_val = y_training_valid[train_index], y_training_valid[val_index]\n",
    "        \n",
    "        xx_bal, yy_bal = ros.fit_resample(xx_train, yy_train)\n",
    "        clf.fit(xx_bal, yy_bal)\n",
    "        fitted_pred = clf.predict(xx_bal)\n",
    "        prec, rec, fscore, supp = precision_recall_fscore_support(y_true=yy_bal, y_pred=fitted_pred, average='binary')\n",
    "        print('training score:', prec, rec, fscore)\n",
    "\n",
    "        preds = clf.predict(xx_val)\n",
    "        prec, rec, fscore, supp = precision_recall_fscore_support(y_true=yy_val, y_pred=preds, average='binary')\n",
    "        acc = accuracy_score(y_true=yy_val, y_pred=preds)\n",
    "        print('validation score:', prec, rec, fscore, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score: 0.8882575757575758 0.9036608863198459 0.8958930276981855\n",
      "validation score: 0.19434306569343066 0.8223938223938224 0.31439114391143913 0.7734146341463415\n",
      "training score: 0.878645343367827 0.9006750241080038 0.8895238095238095\n",
      "validation score: 0.180073126142596 0.7576923076923077 0.29098966026587886 0.7658536585365854\n",
      "training score: 0.8709073900841908 0.8977820636451301 0.8841405508072175\n",
      "validation score: 0.19941634241245138 0.7884615384615384 0.3183229813664596 0.7858536585365854\n",
      "training score: 0.8782771535580525 0.9036608863198459 0.8907882241215574\n",
      "validation score: 0.1894150417827298 0.7876447876447876 0.3053892215568862 0.7736033178824103\n",
      "training score: 0.8711484593837535 0.8988439306358381 0.8847795163584637\n",
      "validation score: 0.18973418881759854 0.7992277992277992 0.3066666666666667 0.7716516223469139\n"
     ]
    }
   ],
   "source": [
    "model_AU1 = LinearSVC(penalty='l2', C=5e-5, loss='squared_hinge', tol=2e-4, max_iter=2000)\n",
    "_run_and_testData(clf=model_AU1,\n",
    "                  x_features=x_features, y_features=y_features, sampling_method='under', cv_n_splits=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Validate the results on benchmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will be only using a small subset of DisfaPlus for testing, as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "disfaP_toy_df = pd.read_csv(\n",
    "    '/home/tiankang/src/feat/dev/disfaP_toy.csv', index_col=0\n",
    ")\n",
    "# This is the csv file that contains filepath and AU labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>aligned_landmark</th>\n",
       "      <th>subject</th>\n",
       "      <th>task</th>\n",
       "      <th>frame</th>\n",
       "      <th>AU1</th>\n",
       "      <th>AU2</th>\n",
       "      <th>AU4</th>\n",
       "      <th>AU5</th>\n",
       "      <th>AU6</th>\n",
       "      <th>AU9</th>\n",
       "      <th>AU12</th>\n",
       "      <th>AU15</th>\n",
       "      <th>AU17</th>\n",
       "      <th>AU20</th>\n",
       "      <th>AU25</th>\n",
       "      <th>AU26</th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33543</th>\n",
       "      <td>5885</td>\n",
       "      <td>[ 42.96360929  43.31080323  53.19024402  38.23...</td>\n",
       "      <td>SN010</td>\n",
       "      <td>Y_SadDescribed_TrailNo_1</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/Storage/Data/DISFAPlusDataset/Images/SN010/Y_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53616</th>\n",
       "      <td>2508</td>\n",
       "      <td>[ 43.97696623  51.17963593  53.27121144  45.34...</td>\n",
       "      <td>SN025</td>\n",
       "      <td>Y_FearDescribed_TrailNo_1</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/Storage/Data/DISFAPlusDataset/Images/SN025/Y_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46436</th>\n",
       "      <td>1824</td>\n",
       "      <td>[ 47.81582882  53.83295079  57.0445239   46.36...</td>\n",
       "      <td>SN001</td>\n",
       "      <td>C1_AU26_TrailNo_2</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/Storage/Data/DISFAPlusDataset/Images/SN001/C1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26573</th>\n",
       "      <td>7612</td>\n",
       "      <td>[ 47.98804535  48.04893453  57.13419986  42.65...</td>\n",
       "      <td>SN027</td>\n",
       "      <td>A3_AU1_2_TrailNo_1</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/Storage/Data/DISFAPlusDataset/Images/SN027/A3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37144</th>\n",
       "      <td>3215</td>\n",
       "      <td>[ 45.78620683  48.71779324  55.53607682  42.76...</td>\n",
       "      <td>SN007</td>\n",
       "      <td>A7_AU5z_TrailNo_2</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/Storage/Data/DISFAPlusDataset/Images/SN007/A7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                   aligned_landmark subject  \\\n",
       "33543        5885  [ 42.96360929  43.31080323  53.19024402  38.23...   SN010   \n",
       "53616        2508  [ 43.97696623  51.17963593  53.27121144  45.34...   SN025   \n",
       "46436        1824  [ 47.81582882  53.83295079  57.0445239   46.36...   SN001   \n",
       "26573        7612  [ 47.98804535  48.04893453  57.13419986  42.65...   SN027   \n",
       "37144        3215  [ 45.78620683  48.71779324  55.53607682  42.76...   SN007   \n",
       "\n",
       "                            task  frame  AU1  AU2  AU4  AU5  AU6  AU9  AU12  \\\n",
       "33543   Y_SadDescribed_TrailNo_1     34    1    1    2    1    0    0     0   \n",
       "53616  Y_FearDescribed_TrailNo_1     74    0    0    0    0    0    0     0   \n",
       "46436          C1_AU26_TrailNo_2     94    0    0    0    0    0    0     0   \n",
       "26573         A3_AU1_2_TrailNo_1     99    0    0    0    0    0    0     0   \n",
       "37144          A7_AU5z_TrailNo_2     32    0    0    0    0    0    0     0   \n",
       "\n",
       "       AU15  AU17  AU20  AU25  AU26  \\\n",
       "33543     0     0     0     0     0   \n",
       "53616     0     0     0     0     0   \n",
       "46436     0     0     0     0     0   \n",
       "26573     0     0     0     0     0   \n",
       "37144     0     0     0     0     0   \n",
       "\n",
       "                                                filepath  \n",
       "33543  /Storage/Data/DISFAPlusDataset/Images/SN010/Y_...  \n",
       "53616  /Storage/Data/DISFAPlusDataset/Images/SN025/Y_...  \n",
       "46436  /Storage/Data/DISFAPlusDataset/Images/SN001/C1...  \n",
       "26573  /Storage/Data/DISFAPlusDataset/Images/SN027/A3...  \n",
       "37144  /Storage/Data/DISFAPlusDataset/Images/SN007/A7...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disfaP_toy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Again use the pyfeat modules to get the HOG and Landmark features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 15.38it/s]\n"
     ]
    }
   ],
   "source": [
    "detector = Detector(face_model='retinaface',emotion_model='resmasknet', landmark_model=\"mobilefacenet\", au_model='svm')\n",
    "\n",
    "SAVE_HOG_DIR = '/Storage/Projects/pyfeat_testing/HOGFeatures/MyHOGTestFeatures/'\n",
    "input_file_list = disfaP_toy_df['filepath'].to_list()\n",
    "\n",
    "if not os.path.exists(SAVE_HOG_DIR):\n",
    "    os.makedirs(SAVE_HOG_DIR)\n",
    "\n",
    "# a list of all path figures\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    ImageDataset(\n",
    "        input_file_list,\n",
    "        output_size=256,\n",
    "        preserve_aspect_ratio=True,\n",
    "        padding=True,\n",
    "    ),\n",
    "    num_workers=0,\n",
    "    batch_size=1,\n",
    "    pin_memory=False,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "def _batch_hog(frames, landmarks):\n",
    "    \"\"\"\n",
    "    Helper function used in batch processing hog features\n",
    "\n",
    "    Args:\n",
    "        frames: a batch of frames\n",
    "        landmarks: a list of list of detected landmarks\n",
    "\n",
    "    Returns:\n",
    "        hog_features: a numpy array of hog features for each detected landmark\n",
    "        landmarks: updated landmarks\n",
    "    \"\"\"\n",
    "    frames = convert_image_to_tensor(frames, img_type=\"float32\")\n",
    "\n",
    "    hog_features = []\n",
    "    hog_images = []\n",
    "    new_landmark_frames = []\n",
    "    for i, frame_landmark in enumerate(landmarks):\n",
    "        if len(frame_landmark) != 0:\n",
    "            new_landmarks_faces = []\n",
    "            for j in range(len(frame_landmark)):\n",
    "                convex_hull, new_landmark = extract_face_from_landmarks(\n",
    "                    frame=frames[i],\n",
    "                    landmarks=frame_landmark[j],\n",
    "                    face_size=112,\n",
    "                )\n",
    "                fd, hog_image=hog(\n",
    "                        transforms.ToPILImage()(convex_hull[0] / 255.0),\n",
    "                        orientations=8,\n",
    "                        pixels_per_cell=(8, 8),\n",
    "                        cells_per_block=(2, 2),\n",
    "                        visualize=True,\n",
    "                        channel_axis=-1,\n",
    "                    )\n",
    "                                \n",
    "                hog_features.append(fd)\n",
    "                hog_images.append(hog_image)\n",
    "                new_landmarks_faces.append(new_landmark)\n",
    "            \n",
    "            new_landmark_frames.append(new_landmarks_faces)\n",
    "        else:\n",
    "            hog_features.append(\n",
    "                np.zeros((1, 5408))\n",
    "            )  # LC: Need to confirm this size is fixed.\n",
    "            new_landmark_frames.append([np.zeros((68, 2))])\n",
    "\n",
    "    return (hog_features, hog_images, new_landmark_frames)\n",
    "\n",
    "\n",
    "for cc, batch_data in enumerate(tqdm(data_loader)):\n",
    "    # Iterate through all the images in dataloader to get the hog feature and landmark feature\n",
    "    try:\n",
    "        faces = detector.detect_faces(\n",
    "                batch_data[\"Image\"],\n",
    "                threshold=0.5)\n",
    "                \n",
    "        landmarks = detector.detect_landmarks(\n",
    "            batch_data[\"Image\"],\n",
    "            detected_faces=faces)\n",
    "\n",
    "        hog_features, hog_images, new_landmark_frames = _batch_hog(batch_data[\"Image\"], landmarks)\n",
    "\n",
    "        for i in range(len(hog_features)):\n",
    "            with open(SAVE_HOG_DIR+batch_data['FileNames'][i].split('/')[-1].split('.')[0]+'.pkl', 'wb') as fp:\n",
    "                pickle.dump((hog_features[i], new_landmark_frames[i][0]), fp)\n",
    "\n",
    "    except:\n",
    "        print('something went wrong with reading the image')\n",
    "        continue;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the HOG & Land Features for test dataset\n",
    "test_hogs, test_land, test_labels_df = compile_dataset(saved_hog_path='/Storage/Projects/pyfeat_testing/HOGFeatures/MyHOGTestFeatures/',\n",
    "                                                        au_df=disfaP_toy_df)\n",
    "test_land = test_land.reshape(test_hogs.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimension reduction on HOG\n",
    "test_data_full_std = scaler_full.transform(test_hogs)\n",
    "test_data_full_transformed = pca_full.transform(test_data_full_std)\n",
    "test_feature = np.concatenate([test_data_full_transformed, test_land], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tested F1 score:  0.41558441558441556\n"
     ]
    }
   ],
   "source": [
    "# Predict and calculate score\n",
    "AU1_predicted = model_AU1.predict(test_feature)\n",
    "AU1_labels = np.where(test_labels_df['AU1'] > 0, 1, 0) # Note that DISFAP uses labels that range from 0 to 5. \n",
    "# We binarize this label to 0 and 1.\n",
    "\n",
    "print(\"tested F1 score: \", f1_score(y_true=AU1_labels, y_pred=AU1_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
