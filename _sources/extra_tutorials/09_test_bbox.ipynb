{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Benchmarking Bounding Box using data\n",
    "*written by Tiankang Xie*  \n",
    "\n",
    "In the tutorial we will demonstrate how to evaluate pyfeat bounding box algorithms with evaluation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from feat.facepose_detectors.img2pose.img2pose_test import Img2Pose\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import glob\n",
    "from feat import Detector\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from feat.facepose_detectors.img2pose.img2pose_test import Img2Pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark script has already been provided by the authors in Matlab. We borrow code from https://github.com/wondervictor/WiderFace-Evaluation, which is a Python version of the original Matlab code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "def voc_ap(rec, prec):\n",
    "\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "    \n",
    "def dataset_pr_info(thresh_num, pr_curve, count_face):\n",
    "    _pr_curve = np.zeros((thresh_num, 2))\n",
    "    for i in range(thresh_num):\n",
    "        _pr_curve[i, 0] = pr_curve[i, 1] / pr_curve[i, 0]\n",
    "        _pr_curve[i, 1] = pr_curve[i, 1] / count_face\n",
    "    return _pr_curve\n",
    "\n",
    "def img_pr_info(thresh_num, pred_info, proposal_list, pred_recall):\n",
    "    pr_info = np.zeros((thresh_num, 2)).astype('float')\n",
    "    for t in range(thresh_num):\n",
    "\n",
    "        thresh = 1 - (t+1)/thresh_num\n",
    "        r_index = np.where(pred_info[:, 4] >= thresh)[0]\n",
    "        if len(r_index) == 0:\n",
    "            pr_info[t, 0] = 0\n",
    "            pr_info[t, 1] = 0\n",
    "        else:\n",
    "            r_index = r_index[-1]\n",
    "            p_index = np.where(proposal_list[:r_index+1] == 1)[0]\n",
    "            pr_info[t, 0] = len(p_index)\n",
    "            pr_info[t, 1] = pred_recall[r_index]\n",
    "    return pr_info\n",
    "\n",
    "def bbox_overlaps(bboxes1,\n",
    "                  bboxes2,\n",
    "                  mode='iou',\n",
    "                  eps=1e-6,\n",
    "                  use_legacy_coordinate=False):\n",
    "    \"\"\"Calculate the ious between each bbox of bboxes1 and bboxes2.\n",
    "    Args:\n",
    "        bboxes1 (ndarray): Shape (n, 4)\n",
    "        bboxes2 (ndarray): Shape (k, 4)\n",
    "        mode (str): IOU (intersection over union) or IOF (intersection\n",
    "            over foreground)\n",
    "        use_legacy_coordinate (bool): Whether to use coordinate system in\n",
    "            mmdet v1.x. which means width, height should be\n",
    "            calculated as 'x2 - x1 + 1` and 'y2 - y1 + 1' respectively.\n",
    "            Note when function is used in `VOCDataset`, it should be\n",
    "            True to align with the official implementation\n",
    "            `http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCdevkit_18-May-2011.tar`\n",
    "            Default: False.\n",
    "    Returns:\n",
    "        ious (ndarray): Shape (n, k)\n",
    "    \"\"\"\n",
    "\n",
    "    assert mode in ['iou', 'iof']\n",
    "    if not use_legacy_coordinate:\n",
    "        extra_length = 0.\n",
    "    else:\n",
    "        extra_length = 1.\n",
    "    bboxes1 = bboxes1.astype(np.float32)\n",
    "    bboxes2 = bboxes2.astype(np.float32)\n",
    "    rows = bboxes1.shape[0]\n",
    "    cols = bboxes2.shape[0]\n",
    "    ious = np.zeros((rows, cols), dtype=np.float32)\n",
    "    if rows * cols == 0:\n",
    "        return ious\n",
    "    exchange = False\n",
    "    if bboxes1.shape[0] > bboxes2.shape[0]:\n",
    "        bboxes1, bboxes2 = bboxes2, bboxes1\n",
    "        ious = np.zeros((cols, rows), dtype=np.float32)\n",
    "        exchange = True\n",
    "    area1 = (bboxes1[:, 2] - bboxes1[:, 0] + extra_length) * (\n",
    "        bboxes1[:, 3] - bboxes1[:, 1] + extra_length)\n",
    "    area2 = (bboxes2[:, 2] - bboxes2[:, 0] + extra_length) * (\n",
    "        bboxes2[:, 3] - bboxes2[:, 1] + extra_length)\n",
    "    for i in range(bboxes1.shape[0]):\n",
    "        x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])\n",
    "        y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])\n",
    "        x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])\n",
    "        y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])\n",
    "        overlap = np.maximum(x_end - x_start + extra_length, 0) * np.maximum(\n",
    "            y_end - y_start + extra_length, 0)\n",
    "        if mode == 'iou':\n",
    "            union = area1[i] + area2 - overlap\n",
    "        else:\n",
    "            union = area1[i] if not exchange else area2\n",
    "        union = np.maximum(union, eps)\n",
    "        ious[i, :] = overlap / union\n",
    "    if exchange:\n",
    "        ious = ious.T\n",
    "    return ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_eval(pred, gt, ignore, iou_thresh):\n",
    "    \"\"\" single image evaluation\n",
    "    pred: Nx5\n",
    "    gt: Nx4\n",
    "    ignore:\n",
    "    \"\"\"\n",
    "    _pred = pred.copy()\n",
    "    _gt = gt.copy()\n",
    "    pred_recall = np.zeros(_pred.shape[0])\n",
    "    recall_list = np.zeros(_gt.shape[0])\n",
    "    proposal_list = np.ones(_pred.shape[0])\n",
    "\n",
    "    # _pred[:, 2] = _pred[:, 2] + _pred[:, 0]\n",
    "    # _pred[:, 3] = _pred[:, 3] + _pred[:, 1]\n",
    "    _gt[:, 2] = _gt[:, 2] + _gt[:, 0]\n",
    "    _gt[:, 3] = _gt[:, 3] + _gt[:, 1]\n",
    "\n",
    "    overlaps = bbox_overlaps(_pred[:, :4], _gt)\n",
    "\n",
    "    for h in range(_pred.shape[0]):\n",
    "\n",
    "        gt_overlap = overlaps[h]\n",
    "        max_overlap, max_idx = gt_overlap.max(), gt_overlap.argmax()\n",
    "        if max_overlap >= iou_thresh:\n",
    "            if ignore[max_idx] == 0:\n",
    "                recall_list[max_idx] = -1\n",
    "                proposal_list[h] = -1\n",
    "            elif recall_list[max_idx] == 0:\n",
    "                recall_list[max_idx] = 1\n",
    "\n",
    "        r_keep_index = np.where(recall_list == 1)[0]\n",
    "        pred_recall[h] = len(r_keep_index)\n",
    "    return pred_recall, proposal_list\n",
    "    \n",
    "def get_gt_boxes(gt_dir):\n",
    "    \"\"\" gt dir: (wider_face_val.mat, wider_easy_val.mat, wider_medium_val.mat, wider_hard_val.mat)\"\"\"\n",
    "\n",
    "    gt_mat = loadmat(os.path.join(gt_dir, 'wider_face_val.mat'))\n",
    "    hard_mat = loadmat(os.path.join(gt_dir, 'wider_hard_val.mat'))\n",
    "    medium_mat = loadmat(os.path.join(gt_dir, 'wider_medium_val.mat'))\n",
    "    easy_mat = loadmat(os.path.join(gt_dir, 'wider_easy_val.mat'))\n",
    "\n",
    "    facebox_list = gt_mat['face_bbx_list']\n",
    "    event_list = gt_mat['event_list']\n",
    "    file_list = gt_mat['file_list']\n",
    "\n",
    "    hard_gt_list = hard_mat['gt_list']\n",
    "    medium_gt_list = medium_mat['gt_list']\n",
    "    easy_gt_list = easy_mat['gt_list']\n",
    "\n",
    "    return facebox_list, event_list, file_list, hard_gt_list, medium_gt_list, easy_gt_list\n",
    "    \n",
    "def load_preds(pred_dir='/Storage/Projects/pyfeat_testing/Data/WIDER_BBOX_IMG2POSE/preds.pkl'):\n",
    "    with open(pred_dir, 'rb') as fp:\n",
    "        all_imgs, all_pred_vals = pickle.load(fp)\n",
    "    boxes = dict()\n",
    "    for i, img_name in enumerate(all_imgs):\n",
    "        event_name = [mai for mai in img_name.split('/') if '--' in mai][0]\n",
    "        if event_name not in boxes:\n",
    "            boxes[event_name] = {}\n",
    "        \n",
    "        pred_box = np.array(all_pred_vals[i])\n",
    "        boxes[event_name][os.path.basename(img_name).rstrip('.jpg')] = pred_box\n",
    "    return boxes\n",
    "\n",
    "def norm_score(pred):\n",
    "    \"\"\" norm score\n",
    "    pred {key: [[x1,y1,x2,y2,s]]}\n",
    "    \"\"\"\n",
    "\n",
    "    max_score = 0\n",
    "    min_score = 1\n",
    "\n",
    "    for _, k in pred.items():\n",
    "        for _, v in k.items():\n",
    "            if len(v.shape) == 0 or len(v) == 0:\n",
    "                continue\n",
    "            _min = np.min(v[:, -1])\n",
    "            _max = np.max(v[:, -1])\n",
    "            max_score = max(_max, max_score)\n",
    "            min_score = min(_min, min_score)\n",
    "\n",
    "    diff = max_score - min_score\n",
    "    for _, k in pred.items():\n",
    "        for _, v in k.items():\n",
    "            if len(v.shape) == 0 or len(v) == 0:\n",
    "                continue\n",
    "            v[:, -1] = (v[:, -1] - min_score)/diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ap_scores(result_fp):\n",
    "    iou_thresh=0.5\n",
    "    pred = load_preds(pred_dir=result_fp) # Where you save the result in the extract_bbox_img2Pose.py script\n",
    "    norm_score(pred)\n",
    "    facebox_list, event_list, file_list, hard_gt_list, medium_gt_list, easy_gt_list = get_gt_boxes(gt_dir='/Storage/Data/wider/wider_face_split/') # Where ground-truth are stored\n",
    "    event_num = len(event_list)\n",
    "    thresh_num = 1000\n",
    "    settings = ['easy', 'medium', 'hard']\n",
    "    setting_gts = [easy_gt_list, medium_gt_list, hard_gt_list]\n",
    "    aps = []\n",
    "    for setting_id in range(3):\n",
    "        # different setting\n",
    "        gt_list = setting_gts[setting_id]\n",
    "        count_face = 0\n",
    "        pr_curve = np.zeros((thresh_num, 2)).astype('float')\n",
    "        # [hard, medium, easy]\n",
    "        pbar = tqdm(range(event_num))\n",
    "        for i in pbar:\n",
    "            pbar.set_description('Processing {}'.format(settings[setting_id]))\n",
    "            event_name = str(event_list[i][0][0])\n",
    "            img_list = file_list[i][0]\n",
    "            pred_list = pred[event_name]\n",
    "            sub_gt_list = gt_list[i][0]\n",
    "            # img_pr_info_list = np.zeros((len(img_list), thresh_num, 2))\n",
    "            gt_bbx_list = facebox_list[i][0]\n",
    "\n",
    "            for j in range(len(img_list)):\n",
    "                pred_info = pred_list[str(img_list[j][0][0])]\n",
    "\n",
    "                gt_boxes = gt_bbx_list[j][0].astype('float')\n",
    "                keep_index = sub_gt_list[j][0]\n",
    "                count_face += len(keep_index)\n",
    "\n",
    "                if len(gt_boxes) == 0 or len(pred_info.shape) == 0 or len(pred_info) == 0:\n",
    "                    continue\n",
    "                ignore = np.zeros(gt_boxes.shape[0])\n",
    "                if len(keep_index) != 0:\n",
    "                    ignore[keep_index-1] = 1\n",
    "                pred_recall, proposal_list = image_eval(pred_info, gt_boxes, ignore, iou_thresh)\n",
    "\n",
    "                _img_pr_info = img_pr_info(thresh_num, pred_info, proposal_list, pred_recall)\n",
    "\n",
    "                pr_curve += _img_pr_info\n",
    "        pr_curve = dataset_pr_info(thresh_num, pr_curve, count_face)\n",
    "\n",
    "        propose = pr_curve[:, 0]\n",
    "        recall = pr_curve[:, 1]\n",
    "\n",
    "        ap = voc_ap(recall, propose)\n",
    "        aps.append(ap)\n",
    "\n",
    "    print(\"==================== Results ====================\")\n",
    "    print(\"Easy   Val AP: {}\".format(aps[0]))\n",
    "    print(\"Medium Val AP: {}\".format(aps[1]))\n",
    "    print(\"Hard   Val AP: {}\".format(aps[2]))\n",
    "    print(\"=================================================\")\n",
    "    return aps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the path for wider dataset, the ground true labels, and where the results are been saved. \n",
    "Dataset and ground truth labels can be downloaded at http://shuoyang1213.me/WIDERFACE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Storage/Data/wider/'\n",
    "true_result_dir = '/Storage/Data/wider/wider_face_split/'\n",
    "save_result_dir = '/Storage/Projects/pyfeat_testing/Data_Eshin/facebox_test/'\n",
    "all_imgs = glob.glob(data_dir+'WIDER_val/images/**/*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test of FaceBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3226/3226 [02:32<00:00, 21.11it/s]\n"
     ]
    }
   ],
   "source": [
    "detector = Detector(face_model='faceboxes',emotion_model='resmasknet', landmark_model='mobilefacenet', au_model='xgb', device='cpu')\n",
    "all_pred_vals = []\n",
    "\n",
    "for img in tqdm(all_imgs):\n",
    "    im1 = Image.open(img)\n",
    "    face_aus = detector.detect_faces(im1)\n",
    "    all_pred_vals.append(face_aus[0])\n",
    "\n",
    "with open(save_result_dir+'FaceBoxes_bench_results.pkl', 'wb') as fp:\n",
    "    pickle.dump((all_imgs, all_pred_vals), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing easy: 100%|██████████| 61/61 [00:11<00:00,  5.33it/s]\n",
      "Processing medium: 100%|██████████| 61/61 [00:11<00:00,  5.33it/s]\n",
      "Processing hard: 100%|██████████| 61/61 [00:11<00:00,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Results ====================\n",
      "Easy   Val AP: 0.5368750176845414\n",
      "Medium Val AP: 0.34812514764839486\n",
      "Hard   Val AP: 0.14662014664396028\n",
      "=================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "facebox_normal = print_ap_scores(result_fp=save_result_dir+'FaceBoxes_bench_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test of MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiankang/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/tiankang/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 3226/3226 [07:13<00:00,  7.44it/s]\n"
     ]
    }
   ],
   "source": [
    "detector = Detector(face_model='mtcnn',emotion_model='resmasknet', landmark_model='mobilefacenet', au_model='xgb', device='cpu')\n",
    "all_pred_vals = []\n",
    "\n",
    "for img in tqdm(all_imgs):\n",
    "    im1 = Image.open(img)\n",
    "    face_aus = detector.detect_faces(im1)\n",
    "    all_pred_vals.append(face_aus[0])\n",
    "\n",
    "with open(save_result_dir+'MTCNN_bench_results.pkl', 'wb') as fp:\n",
    "    pickle.dump((all_imgs, all_pred_vals), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing easy: 100%|██████████| 61/61 [00:16<00:00,  3.77it/s]\n",
      "Processing medium: 100%|██████████| 61/61 [00:16<00:00,  3.75it/s]\n",
      "Processing hard: 100%|██████████| 61/61 [00:16<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Results ====================\n",
      "Easy   Val AP: 0.7248933447919402\n",
      "Medium Val AP: 0.7175922904388756\n",
      "Hard   Val AP: 0.47326227608164284\n",
      "=================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mtcnn_normal = print_ap_scores(result_fp=save_result_dir+'MTCNN_bench_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test of RetinaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiankang/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/tiankang/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  6%|▌         | 178/3226 [00:22<05:13,  9.72it/s]"
     ]
    }
   ],
   "source": [
    "detector = Detector(face_model='retinaface',emotion_model='resmasknet', landmark_model='mobilefacenet', au_model='xgb', device='cpu')\n",
    "all_pred_vals = []\n",
    "\n",
    "for img in tqdm(all_imgs):\n",
    "    im1 = Image.open(img)\n",
    "    face_aus = detector.detect_faces(im1)\n",
    "    all_pred_vals.append(face_aus[0])\n",
    "\n",
    "with open(save_result_dir+'RetinaFace_bench_results.pkl', 'wb') as fp:\n",
    "    pickle.dump((all_imgs, all_pred_vals), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinaface_normal = print_ap_scores(result_fp=save_result_dir+'RetinaFace_bench_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test of Img2Pose unconstrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Img2Pose models are heavy both in architecture and in number of hyperparameters. We advise to use different parameter combinations for different settings, especially for constrained vs unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feat.facepose_detectors.img2pose.img2pose_test import Img2Pose\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericImageDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.filePaths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filePaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.filePaths[idx])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3226/3226 [01:48<00:00, 29.82it/s]\n"
     ]
    }
   ],
   "source": [
    "imclassifier = Img2Pose(constrained=False, detection_threshold=0.25, rpn_pre_nms_top_n_test=6000, rpn_post_nms_top_n_test=1000)\n",
    "\n",
    "img_trans = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = GenericImageDataset(all_imgs, transform=img_trans)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "all_pred_vals = []\n",
    "for i_batch, sample_batched in enumerate(tqdm(dataloader)):\n",
    "    preds = imclassifier(sample_batched)\n",
    "    all_pred_vals.append(preds[0][0]) # Append Face Bounding Box\n",
    "\n",
    "# Save Result\n",
    "with open(save_result_dir+'Img2poseuncon_bench_results.pkl', 'wb') as fp:\n",
    "    pickle.dump((all_imgs, all_pred_vals), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing easy: 100%|██████████| 61/61 [00:19<00:00,  3.18it/s]\n",
      "Processing medium: 100%|██████████| 61/61 [00:19<00:00,  3.20it/s]\n",
      "Processing hard: 100%|██████████| 61/61 [00:18<00:00,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Results ====================\n",
      "Easy   Val AP: 0.8563027227316843\n",
      "Medium Val AP: 0.8136765059086696\n",
      "Hard   Val AP: 0.5739961257745989\n",
      "=================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img2pose_uncon_normal = print_ap_scores(result_fp=save_result_dir+'Img2poseuncon_bench_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test of Img2Pose constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3226/3226 [01:13<00:00, 43.72it/s]\n"
     ]
    }
   ],
   "source": [
    "imclassifier = Img2Pose(constrained=True, detection_threshold=0.25, rpn_pre_nms_top_n_test=2000, rpn_post_nms_top_n_test=200)\n",
    "\n",
    "img_trans = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = GenericImageDataset(all_imgs, transform=img_trans)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "all_pred_vals = []\n",
    "for i_batch, sample_batched in enumerate(tqdm(dataloader)):\n",
    "    preds = imclassifier(sample_batched)\n",
    "    all_pred_vals.append(preds[0][0]) # Append Face Bounding Box\n",
    "\n",
    "# Save Result\n",
    "with open(save_result_dir+'Img2posecon_bench_results.pkl', 'wb') as fp:\n",
    "    pickle.dump((all_imgs, all_pred_vals), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing easy: 100%|██████████| 61/61 [00:16<00:00,  3.60it/s]\n",
      "Processing medium: 100%|██████████| 61/61 [00:17<00:00,  3.56it/s]\n",
      "Processing hard: 100%|██████████| 61/61 [00:17<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Results ====================\n",
      "Easy   Val AP: 0.6470359115773076\n",
      "Medium Val AP: 0.5878560483278932\n",
      "Hard   Val AP: 0.32415904798673495\n",
      "=================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "img2pose_con_normal = print_ap_scores(result_fp=save_result_dir+'Img2posecon_bench_results.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb973b375f2d280323e3fea5f1f02d1878ea0342ecfe5636a4609970d8a89fff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
