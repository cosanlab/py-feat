{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Example labels and landmark dataset loading\n",
    "*written by Eshin Jolly*\n",
    "\n",
    "This notebook demonstrates how we extracted labels and landmarks from the EmotioNet dataset. You'll want to repeat this for the DISFA Plus and BP4d datasets prior to [training the AU visualization model](./06_trainAUvisModel.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-26T04:43:45.310255Z",
     "start_time": "2021-03-26T04:43:45.307443Z"
    }
   },
   "source": [
    "## Imports, paths, and helper functions\n",
    "Make sure to adjust data paths as needed. By default this notebook assumes datasets are in the `data/datasets` folder at the root of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import math, cv2, csv\n",
    "from scipy.spatial import ConvexHull\n",
    "from skimage.morphology.convex_hull import grid_points_in_poly\n",
    "from feat import Detector\n",
    "import os, glob, pandas as pd, numpy as np\n",
    "from skimage.feature import hog\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set data directory to data folder relative to location of this notebook\n",
    "data_dir = os.path.join(\n",
    "    os.path.dirname(os.path.dirname(os.path.realpath(\"\"))), \"data\", \"datasets\"\n",
    ")\n",
    "\n",
    "\n",
    "def padding(img, expected_size):\n",
    "    desired_size = expected_size\n",
    "    delta_width = desired_size - img.size[0]\n",
    "    delta_height = desired_size - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (\n",
    "        pad_width,\n",
    "        pad_height,\n",
    "        delta_width - pad_width,\n",
    "        delta_height - pad_height,\n",
    "    )\n",
    "    return ImageOps.expand(img, padding)\n",
    "\n",
    "\n",
    "def resize_with_padding(img, expected_size):\n",
    "    img.thumbnail((expected_size[0], expected_size[1]))\n",
    "    delta_width = expected_size[0] - img.size[0]\n",
    "    delta_height = expected_size[1] - img.size[1]\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    padding = (\n",
    "        pad_width,\n",
    "        pad_height,\n",
    "        delta_width - pad_width,\n",
    "        delta_height - pad_height,\n",
    "    )\n",
    "    return ImageOps.expand(img, padding)\n",
    "\n",
    "\n",
    "def align_face_68pts(img, img_land, box_enlarge, img_size=112):\n",
    "    \"\"\"\n",
    "    img: image\n",
    "    img_land: landmarks 68\n",
    "    box_enlarge: relative size of face\n",
    "    img_size = 112\n",
    "\n",
    "    \"\"\"\n",
    "    leftEye0 = (\n",
    "        img_land[2 * 36]\n",
    "        + img_land[2 * 37]\n",
    "        + img_land[2 * 38]\n",
    "        + img_land[2 * 39]\n",
    "        + img_land[2 * 40]\n",
    "        + img_land[2 * 41]\n",
    "    ) / 6.0\n",
    "    leftEye1 = (\n",
    "        img_land[2 * 36 + 1]\n",
    "        + img_land[2 * 37 + 1]\n",
    "        + img_land[2 * 38 + 1]\n",
    "        + img_land[2 * 39 + 1]\n",
    "        + img_land[2 * 40 + 1]\n",
    "        + img_land[2 * 41 + 1]\n",
    "    ) / 6.0\n",
    "    rightEye0 = (\n",
    "        img_land[2 * 42]\n",
    "        + img_land[2 * 43]\n",
    "        + img_land[2 * 44]\n",
    "        + img_land[2 * 45]\n",
    "        + img_land[2 * 46]\n",
    "        + img_land[2 * 47]\n",
    "    ) / 6.0\n",
    "    rightEye1 = (\n",
    "        img_land[2 * 42 + 1]\n",
    "        + img_land[2 * 43 + 1]\n",
    "        + img_land[2 * 44 + 1]\n",
    "        + img_land[2 * 45 + 1]\n",
    "        + img_land[2 * 46 + 1]\n",
    "        + img_land[2 * 47 + 1]\n",
    "    ) / 6.0\n",
    "    deltaX = rightEye0 - leftEye0\n",
    "    deltaY = rightEye1 - leftEye1\n",
    "    l = math.sqrt(deltaX * deltaX + deltaY * deltaY)\n",
    "    sinVal = deltaY / l\n",
    "    cosVal = deltaX / l\n",
    "    mat1 = np.mat([[cosVal, sinVal, 0], [-sinVal, cosVal, 0], [0, 0, 1]])\n",
    "    mat2 = np.mat(\n",
    "        [\n",
    "            [leftEye0, leftEye1, 1],\n",
    "            [rightEye0, rightEye1, 1],\n",
    "            [img_land[2 * 30], img_land[2 * 30 + 1], 1],\n",
    "            [img_land[2 * 48], img_land[2 * 48 + 1], 1],\n",
    "            [img_land[2 * 54], img_land[2 * 54 + 1], 1],\n",
    "        ]\n",
    "    )\n",
    "    mat2 = (mat1 * mat2.T).T\n",
    "    cx = float((max(mat2[:, 0]) + min(mat2[:, 0]))) * 0.5\n",
    "    cy = float((max(mat2[:, 1]) + min(mat2[:, 1]))) * 0.5\n",
    "    if float(max(mat2[:, 0]) - min(mat2[:, 0])) > float(\n",
    "        max(mat2[:, 1]) - min(mat2[:, 1])\n",
    "    ):\n",
    "        halfSize = 0.5 * box_enlarge * float((max(mat2[:, 0]) - min(mat2[:, 0])))\n",
    "    else:\n",
    "        halfSize = 0.5 * box_enlarge * float((max(mat2[:, 1]) - min(mat2[:, 1])))\n",
    "    scale = (img_size - 1) / 2.0 / halfSize\n",
    "    mat3 = np.mat(\n",
    "        [\n",
    "            [scale, 0, scale * (halfSize - cx)],\n",
    "            [0, scale, scale * (halfSize - cy)],\n",
    "            [0, 0, 1],\n",
    "        ]\n",
    "    )\n",
    "    mat = mat3 * mat1\n",
    "    aligned_img = cv2.warpAffine(\n",
    "        img,\n",
    "        mat[0:2, :],\n",
    "        (img_size, img_size),\n",
    "        cv2.INTER_LINEAR,\n",
    "        borderValue=(128, 128, 128),\n",
    "    )\n",
    "    land_3d = np.ones((int(len(img_land) / 2), 3))\n",
    "    land_3d[:, 0:2] = np.reshape(np.array(img_land), (int(len(img_land) / 2), 2))\n",
    "    mat_land_3d = np.mat(land_3d)\n",
    "    new_land = np.array((mat * mat_land_3d.T).T)\n",
    "    new_land = np.array(list(zip(new_land[:, 0], new_land[:, 1]))).astype(int)\n",
    "    return aligned_img, new_land\n",
    "\n",
    "\n",
    "def extract_hog(image, detector):\n",
    "    im = cv2.imread(image)\n",
    "    detected_faces = np.array(detector.detect_faces(im)[0])\n",
    "    if np.any(detected_faces < 0):\n",
    "        orig_size = np.array(im).shape\n",
    "        if np.where(detected_faces < 0)[0][0] == 1:\n",
    "            new_size = (\n",
    "                orig_size[0],\n",
    "                int(orig_size[1] + 2 * abs(detected_faces[detected_faces < 0][0])),\n",
    "            )\n",
    "        else:\n",
    "            new_size = (\n",
    "                int(orig_size[0] + 2 * abs(detected_faces[detected_faces < 0][0])),\n",
    "                orig_size[1],\n",
    "            )\n",
    "        im = resize_with_padding(Image.fromarray(im), new_size)\n",
    "        im = np.asarray(im)\n",
    "        detected_faces = np.array(detector.detect_faces(np.array(im))[0])\n",
    "    detected_faces = detected_faces.astype(int)\n",
    "    points = detector.detect_landmarks(np.array(im), [detected_faces])[0].astype(int)\n",
    "\n",
    "    aligned_img, points = align_face_68pts(im, points.flatten(), 2.5)\n",
    "\n",
    "    hull = ConvexHull(points)\n",
    "    mask = grid_points_in_poly(\n",
    "        shape=np.array(aligned_img).shape,\n",
    "        verts=list(\n",
    "            zip(points[hull.vertices][:, 1], points[hull.vertices][:, 0])\n",
    "        ),  # for some reason verts need to be flipped\n",
    "    )\n",
    "\n",
    "    mask[0 : np.min([points[0][1], points[16][1]]), points[0][0] : points[16][0]] = True\n",
    "    aligned_img[~mask] = 0\n",
    "    resized_face_np = aligned_img\n",
    "\n",
    "    fd, hog_image = hog(\n",
    "        resized_face_np,\n",
    "        orientations=8,\n",
    "        pixels_per_cell=(8, 8),\n",
    "        cells_per_block=(2, 2),\n",
    "        visualize=True,\n",
    "        multichannel=True,\n",
    "    )\n",
    "\n",
    "    return fd, hog_image, points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example extraction on EmotioNet\n",
    "\n",
    "You'll need to run this for DISFA Plus and BP4d as well in order to [train our AU visualization model](./06_trainAUvisModel.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"EmotionNet\"\n",
    "labels_filename = \"emotionet_labels.csv\"\n",
    "landmarks_filename = \"emotionet_landmarks.csv\"\n",
    "\n",
    "detector = Detector(face_model=\"retinaface\", landmark_model=\"mobilenet\")\n",
    "EmotioNet_images = np.sort(glob.glob(os.path.join(data_dir, dataset, \"imgs\", \"*.jpg\")))\n",
    "labels = pd.read_csv(\n",
    "    os.path.join(data_dir, dataset, \"labels\", \"EmotioNet_FACS_aws_2020_24600.csv\")\n",
    ")\n",
    "labels = labels.dropna(axis=0)\n",
    "for col in labels.columns:\n",
    "    if \"AU\" in col:\n",
    "        kwargs = {col.replace(\"'\", \"\").replace('\"', \"\").replace(\" \", \"\"): labels[[col]]}\n",
    "        labels = labels.assign(**kwargs)\n",
    "        labels = labels.drop(columns=col)\n",
    "labels = labels.assign(\n",
    "    URL=labels.URL.apply(lambda x: x.split(\"/\")[-1].replace(\"'\", \"\"))\n",
    ")\n",
    "labels = labels.set_index(\"URL\")\n",
    "labels = labels.drop(columns=[\"URL orig\"])\n",
    "\n",
    "aus_to_train = [\n",
    "    \"AU1\",\n",
    "    \"AU2\",\n",
    "    \"AU4\",\n",
    "    \"AU5\",\n",
    "    \"AU6\",\n",
    "    \"AU7\",\n",
    "    \"AU9\",\n",
    "    \"AU10\",\n",
    "    \"AU11\",\n",
    "    \"AU12\",\n",
    "    \"AU14\",\n",
    "    \"AU15\",\n",
    "    \"AU17\",\n",
    "    \"AU20\",\n",
    "    \"AU23\",\n",
    "    \"AU24\",\n",
    "    \"AU25\",\n",
    "    \"AU26\",\n",
    "    \"AU28\",\n",
    "    \"AU43\",\n",
    "]\n",
    "\n",
    "with open(labels_filename, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=\",\")\n",
    "    writer.writerow([\"URL\"] + aus_to_train)\n",
    "\n",
    "landmark_cols = [f\"x_{i}\" for i in range(68)] + [f\"y_{i}\" for i in range(68)]\n",
    "with open(landmarks_filename, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=\",\")\n",
    "    writer.writerow(landmark_cols)\n",
    "\n",
    "for ix, image in enumerate(tqdm(EmotioNet_images)):\n",
    "    try:\n",
    "        imageURL = os.path.split(image)[-1]\n",
    "        label = labels.loc[imageURL][aus_to_train]\n",
    "        fd, _, points = extract_hog(image, detector=detector)\n",
    "        with open(labels_filename, \"a+\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=\",\")\n",
    "            writer.writerow([imageURL] + list(label.values))\n",
    "        with open(landmarks_filename, \"a+\", newline=\"\") as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=\",\")\n",
    "            writer.writerow(points.T.flatten())\n",
    "    except:\n",
    "        print(f\"failed {image}\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py-feat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "7dcf886e642ffd7132d8e9a6cd5ca71978ba2253d781a5b1b4944468a6c69f78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
