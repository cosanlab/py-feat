{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Benchmarking Pose detectors using data\n",
    "*written by Tiankang Xie*  \n",
    "\n",
    "In the tutorial we will demonstrate how to evaluate pyfeat bounding pose detection algorithms with evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "# Extract Pose information from \n",
    "from feat.facepose_detectors.img2pose.img2pose_test import Img2Pose\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from feat import Detector\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation\n",
    "from feat import Detector\n",
    "\n",
    "def convert_to_aflw(rotvec, is_rotvec=True):\n",
    "    if is_rotvec:\n",
    "        rotvec = Rotation.from_rotvec(rotvec).as_matrix()\n",
    "    rot_mat_2 = np.transpose(rotvec)\n",
    "    angle = Rotation.from_matrix(rot_mat_2).as_euler('xyz', degrees=True)\n",
    "    \n",
    "    return np.array([angle[0], -angle[1], -angle[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericImageDataset(Dataset):\n",
    "    \"\"\"Generic Image dataset loader for PyTorch.\"\"\"\n",
    "\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.filePaths = file_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filePaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.filePaths[idx])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide the path for the data. It can be downloaded from https://www.kaggle.com/datasets/kmader/biwi-kinect-head-pose-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/Storage/Data/Kinect_Pose/hpdb/'\n",
    "all_imgs_paths = glob.glob(data_dir+'**/*.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tiankang/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'backbone_name' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/tiankang/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 15678/15678 [05:50<00:00, 44.78it/s]\n",
      "100%|██████████| 15678/15678 [00:00<00:00, 183154.93it/s]\n"
     ]
    }
   ],
   "source": [
    "imclassifier = Img2Pose(constrained=False, detection_threshold=0.25, rpn_pre_nms_top_n_test=6000, rpn_post_nms_top_n_test=1000)\n",
    "\n",
    "img_trans = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = GenericImageDataset(all_imgs_paths, transform=img_trans)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "all_pred_vals = []\n",
    "for i_batch, sample_batched in enumerate(tqdm(dataloader)):\n",
    "    preds = imclassifier(sample_batched)\n",
    "    all_pred_vals.append(preds[1])\n",
    "\n",
    "pose_targets = []\n",
    "for img_path in all_imgs_paths:\n",
    "    annotations  = open(img_path.replace(\"_rgb.png\", \"_pose.txt\"))\n",
    "    lines = annotations.readlines()\n",
    "\n",
    "    pose_target = []\n",
    "    for i in range(3):\n",
    "        lines[i] = str(lines[i].rstrip(\"\\n\")) \n",
    "        pose_target.append(lines[i].split(\" \")[:3])\n",
    "    pose_target = np.asarray(pose_target).astype(float)     \n",
    "    pose_target = convert_to_aflw(pose_target, False)\n",
    "    pose_targets.append(pose_target)\n",
    "pose_target_arr = np.asarray(pose_targets)#[np.array(invalids), :]\n",
    "pose_target_arr[:, [1, 2]] = pose_target_arr[:, [2, 1]]\n",
    "\n",
    "new_arr = []\n",
    "invalids = []\n",
    "for i, arrr1 in enumerate(tqdm(all_pred_vals)):\n",
    "    if len(arrr1[0]) > 0: \n",
    "        if len(arrr1[0]) == 1: \n",
    "            new_arr.append(arrr1[0][0][0])\n",
    "            invalids.append(i)\n",
    "        else:\n",
    "            vv = []\n",
    "            for part_arr in arrr1[0]:\n",
    "                val = np.sum(abs(part_arr - pose_target_arr[i]))\n",
    "                vv.append(val)\n",
    "            argmin = np.argmin(vv)\n",
    "            new_arr.append(arrr1[0][argmin][0])\n",
    "            invalids.append(i)\n",
    "    else:\n",
    "        continue;\n",
    "new_arr = np.vstack(new_arr)\n",
    "\n",
    "pose_ae = abs(new_arr - pose_target_arr[invalids,:])\n",
    "pose_ae_summed = np.sum(pose_ae, axis=1)\n",
    "neg_index = np.where(pose_ae_summed>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitch, yaw, roll scores:\n",
      "[6.25469727 3.38062612 4.54291059]\n",
      "average grand mean\n",
      "4.726077992404776\n"
     ]
    }
   ],
   "source": [
    "print('pitch, yaw, roll error in degrees:')\n",
    "print(np.mean(pose_ae,0))\n",
    "print('average grand mean')\n",
    "print(np.mean(pose_ae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15678/15678 [04:39<00:00, 56.12it/s]\n",
      "100%|██████████| 15678/15678 [00:00<00:00, 123273.54it/s]\n"
     ]
    }
   ],
   "source": [
    "imclassifier = Img2Pose(constrained=True, detection_threshold=0.25, rpn_pre_nms_top_n_test=2000, rpn_post_nms_top_n_test=200)\n",
    "\n",
    "img_trans = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = GenericImageDataset(all_imgs_paths, transform=img_trans)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "all_pred_vals = []\n",
    "for i_batch, sample_batched in enumerate(tqdm(dataloader)):\n",
    "    preds = imclassifier(sample_batched)\n",
    "    all_pred_vals.append(preds[1])\n",
    "\n",
    "pose_targets = []\n",
    "for img_path in all_imgs_paths:\n",
    "    annotations  = open(img_path.replace(\"_rgb.png\", \"_pose.txt\"))\n",
    "    lines = annotations.readlines()\n",
    "\n",
    "    pose_target = []\n",
    "    for i in range(3):\n",
    "        lines[i] = str(lines[i].rstrip(\"\\n\")) \n",
    "        pose_target.append(lines[i].split(\" \")[:3])\n",
    "    pose_target = np.asarray(pose_target).astype(float)     \n",
    "    pose_target = convert_to_aflw(pose_target, False)\n",
    "    pose_targets.append(pose_target)\n",
    "pose_target_arr = np.asarray(pose_targets)#[np.array(invalids), :]\n",
    "pose_target_arr[:, [1, 2]] = pose_target_arr[:, [2, 1]]\n",
    "\n",
    "new_arr = []\n",
    "invalids = []\n",
    "for i, arrr1 in enumerate(tqdm(all_pred_vals)):\n",
    "    if len(arrr1[0]) > 0: \n",
    "        if len(arrr1[0]) == 1: \n",
    "            new_arr.append(arrr1[0][0][0])\n",
    "            invalids.append(i)\n",
    "        else:\n",
    "            vv = []\n",
    "            for part_arr in arrr1[0]:\n",
    "                val = np.sum(abs(part_arr - pose_target_arr[i]))\n",
    "                vv.append(val)\n",
    "            argmin = np.argmin(vv)\n",
    "            new_arr.append(arrr1[0][argmin][0])\n",
    "            invalids.append(i)\n",
    "    else:\n",
    "        continue;\n",
    "new_arr = np.vstack(new_arr)\n",
    "\n",
    "pose_ae = abs(new_arr - pose_target_arr[invalids,:])\n",
    "pose_ae_summed = np.sum(pose_ae, axis=1)\n",
    "neg_index = np.where(pose_ae_summed>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pitch, yaw, roll scores:\n",
      "[4.56942555 3.39055895 4.53926361]\n",
      "average grand mean\n",
      "4.16641603923617\n"
     ]
    }
   ],
   "source": [
    "print('pitch, yaw, roll scores in degrees:')\n",
    "print(np.mean(pose_ae,0))\n",
    "print('average grand mean')\n",
    "print(np.mean(pose_ae))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb973b375f2d280323e3fea5f1f02d1878ea0342ecfe5636a4609970d8a89fff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
