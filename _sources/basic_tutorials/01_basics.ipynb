{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Py-Feat basics\n",
    "\n",
    "*Written by Eshin Jolly*\n",
    "\n",
    "This tutorial goes over the basics of using Py-Feat's API. You can try it out interactively in Google Collab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cosanlab/py-feat/blob/master/notebooks/content/01_basics.ipynb)\n",
    "\n",
    "At a broad level you will be working with 2 main objects in Py-Feat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below and run this only if you're using Google Collab\n",
    "# !pip install -q py-feat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectors\n",
    "\n",
    "A detector is a swiss-army-knife class that \"glues\" together a particular combination of a Face, Landmark, Action Unit, and Emotion detection model into a single object. This allows us to provide a very easy-to-use high-level API, e.g. `detector.detect_image('my_image.jpg')`, which will automatically make use of the correct underlying model to solve the sub-tasks of identifying face locations, getting landmarks, extracting action units, etc. \n",
    "\n",
    "The first time you initialize a `Detector` instance on your computer will take a moment as Py-Feat will automatically download required pretrained model weights for you and save them to disk. Everytime after that it will use existing model weights:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/mobilenet0.25_Final.pth\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/mobilenet_224_model_best_gdconv_external.pth.tar\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/svm_60_Nov22022.pkl\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/upper_face_pcaSet.pkl\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/lower_face_pcaSet.pkl\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/full_face_pcaSet.pkl\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/ResMaskNet_Z_resmasking_dropout1_rot30.pth\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/ResMaskNet_fer2013_config.json\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/reference_3d_68_points_trans.npy\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/img2pose_v1.pth\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/WIDER_train_pose_mean_v1.npy\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/WIDER_train_pose_stddev_v1.npy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feat.detector.Detector(face_model=retinaface, landmark_model=mobilenet, au_model=svm, emotion_model=resmasknet, facepose_model=img2pose)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from feat.detector import Detector\n",
    "\n",
    "# The verbose flag will print out messages indicating whether a pre-trained model\n",
    "# is being downloaded for the first time, or being loaded from disk.\n",
    "# In this case I already have all the models downloaded, but if you run this notebook on\n",
    "# your personal computer or Google Collab, these will be downloaded the first time you\n",
    "# use a Detector.\n",
    "detector = Detector(verbose=True)\n",
    "\n",
    "detector"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default detectors\n",
    "\n",
    "The default models loaded by Py-Feat are the currently recommended ones for each detection task. You see a full list of supported models [here](../pages/models.md) and instructions for contributing a new detector [here](../pages/modelContribution.md). The current defaults include:\n",
    "- Face Model: `retinaface`\n",
    "- Landmark Model: `mobilefacenet`\n",
    "- AU Model: `svm`\n",
    "- Emotion Model: `resmasknet`\n",
    "- Face Pose Model:`img2pose`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing a detector you can easily swap one or more underlying models using the `.change_model` method. All models names should be provided as strings and are case-insensitive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/onet.pt\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/pnet.pt\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/rnet.pt\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/mobilenet_224_model_best_gdconv_external.pth.tar\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/svm_60_Nov22022.pkl\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/upper_face_pcaSet.pkl\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/lower_face_pcaSet.pkl\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/full_face_pcaSet.pkl\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/ResMaskNet_Z_resmasking_dropout1_rot30.pth\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/ResMaskNet_fer2013_config.json\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/reference_3d_68_points_trans.npy\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/img2pose_v1.pth\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/WIDER_train_pose_mean_v1.npy\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/WIDER_train_pose_stddev_v1.npy\n",
      "Changing face_model from retinaface -> mtcnn\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feat.detector.Detector(face_model=mtcnn, landmark_model=mobilenet, au_model=svm, emotion_model=resmasknet, facepose_model=img2pose)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector.change_model(face_model=\"MTCNN\")\n",
    "detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll primarily use a detector instance by calling its `.detect_image()` or `.detect_video()` methods. Both methods take as input a filename (or list of filenames) and return a `Fex` data class instance. Detectors also have lower-level detection methods like `.detect_face()` and `.detect_landmarks()` which operate on and return numpy arrays directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fex data classes\n",
    "A Fex data class is just a special type of pandas dataframe then makes it easier to work with the results returned by a detector. Each row contains information about a detected face and each column contains the output of that detection (e.g. x, y location; emotion) or some meta-data about the input file (e.g. the filename). \n",
    "\n",
    "Fex data instances have helper methods on them to quickly retrieve the appropriate data you want without having to search through column names yourself, e.g.  `fex.emotions()`. They also have methods for plotting e.g. `fex.plot_detections()`, signal-processing e.g. `fex.downsample()`, and statistical analysis e.g. `fex.regress()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from feat.data import Fex\n",
    "import pandas as pd\n",
    "\n",
    "fex = Fex()\n",
    "\n",
    "isinstance(fex, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More often than not, you'll be using `Fex` data classes that are return by `Detector` rather than creating them from scratch. Check out the next tutorial for more details on how this works!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7dcf886e642ffd7132d8e9a6cd5ca71978ba2253d781a5b1b4944468a6c69f78"
  },
  "kernelspec": {
   "display_name": "py-feat",
   "language": "python",
   "name": "py-feat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Oct 19 2022, 17:54:22) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
