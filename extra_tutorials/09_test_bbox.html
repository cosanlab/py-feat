
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>9. Benchmarking Bounding Box using data &#8212; Py-Feat</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="10. Benchmarking Landmark models using data" href="10_test_lands.html" />
    <link rel="prev" title="8. Training HOG-based AU detectors" href="08_train_hogs.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/pyfeat_logo_small.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Py-Feat</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../pages/intro.html">
                    Py-Feat: Python Facial Expression Analysis Toolbox
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../pages/installation.html">
   How to install Py-Feat
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pages/models.html">
   Included pre-trained detectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pages/au_reference.html">
   Action Unit Reference
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basic Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../basic_tutorials/01_basics.html">
   1. Py-Feat basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../basic_tutorials/02_detector_imgs.html">
   2. Detecting facial expressions from images
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../basic_tutorials/03_detector_vids.html">
   3. Detecting facial expressions from videos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../basic_tutorials/04_plotting.html">
   4. Visualizing Facial Expressions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../basic_tutorials/05_fex_analysis.html">
   5. Running a full analysis
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Advanced Tutorials
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="06_trainAUvisModel.html">
   6. Training an AU visualization model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07_extract_labels_and_landmarks.html">
   7. Example labels and landmark dataset loading
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08_train_hogs.html">
   8. Training HOG-based AU detectors
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   9. Benchmarking Bounding Box using data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="10_test_lands.html">
   10. Benchmarking Landmark models using data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="11_test_Poseinfo.html">
   11. Benchmarking Pose detectors using data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="12_test_aus.html">
   12. Benchmarking Action Unit detector using data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="13_test_emos.html">
   13. Benchmarking pyfeat Emotion detection algorithms using data
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  API
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../pages/api.html">
   API Reference
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../pages/contribute.html">
   General contributions guidelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pages/modelContribution.html">
   Contributing new detectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pages/changelog.html">
   Change Log
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/cosanlab/py-feat">
   GitHub Repository
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/cosanlab/py-feat/master?urlpath=tree/notebooks/extra_tutorials/09_test_bbox.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://jhub.dartmouth.edu/hub/user-redirect/git-pull?repo=https%3A//github.com/cosanlab/py-feat&urlpath=tree/py-feat/notebooks/extra_tutorials/09_test_bbox.ipynb&branch=master"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on JupyterHub"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_jupyterhub.svg">
  </span>
<span class="headerbtn__text-container">JupyterHub</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/cosanlab/py-feat/blob/master/notebooks/extra_tutorials/09_test_bbox.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/cosanlab/py-feat"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/cosanlab/py-feat/issues/new?title=Issue%20on%20page%20%2Fextra_tutorials/09_test_bbox.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/extra_tutorials/09_test_bbox.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   9. Benchmarking Bounding Box using data
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#test-of-faceboxes">
   1. Test of FaceBoxes
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#test-of-mtcnn">
   2. Test of MTCNN
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#test-of-retinaface">
   3. Test of RetinaFace
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#test-of-img2pose-unconstrained">
   4. Test of Img2Pose unconstrained
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#img2pose-models-are-heavy-both-in-architecture-and-in-number-of-hyperparameters-we-advise-to-use-different-parameter-combinations-for-different-settings-especially-for-constrained-vs-unconstrained">
     Img2Pose models are heavy both in architecture and in number of hyperparameters. We advise to use different parameter combinations for different settings, especially for constrained vs unconstrained
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#test-of-img2pose-constrained">
   5. Test of Img2Pose constrained
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>9. Benchmarking Bounding Box using data</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   9. Benchmarking Bounding Box using data
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#test-of-faceboxes">
   1. Test of FaceBoxes
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#test-of-mtcnn">
   2. Test of MTCNN
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#test-of-retinaface">
   3. Test of RetinaFace
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#test-of-img2pose-unconstrained">
   4. Test of Img2Pose unconstrained
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#img2pose-models-are-heavy-both-in-architecture-and-in-number-of-hyperparameters-we-advise-to-use-different-parameter-combinations-for-different-settings-especially-for-constrained-vs-unconstrained">
     Img2Pose models are heavy both in architecture and in number of hyperparameters. We advise to use different parameter combinations for different settings, especially for constrained vs unconstrained
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#test-of-img2pose-constrained">
   5. Test of Img2Pose constrained
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="benchmarking-bounding-box-using-data">
<h1>9. Benchmarking Bounding Box using data<a class="headerlink" href="#benchmarking-bounding-box-using-data" title="Permalink to this headline">#</a></h1>
<p><em>written by Tiankang Xie</em></p>
<p>In the tutorial we will demonstrate how to evaluate pyfeat bounding box algorithms with evaluation data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import pickle
import numpy as np
import os
from scipy.io import loadmat
from tqdm import tqdm
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt
import pandas as pd

from feat.facepose_detectors.img2pose.img2pose_test import Img2Pose
from PIL import Image, ImageDraw
import torchvision.transforms as transforms
import torch
import glob
from feat import Detector
import matplotlib.pyplot as plt

from feat.facepose_detectors.img2pose.img2pose_test import Img2Pose
</pre></div>
</div>
</div>
</div>
<p>The benchmark script has already been provided by the authors in Matlab. We borrow code from https://github.com/wondervictor/WiderFace-Evaluation, which is a Python version of the original Matlab code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Copyright (c) OpenMMLab. All rights reserved.
def voc_ap(rec, prec):

    # correct AP calculation
    # first append sentinel values at the end
    mrec = np.concatenate(([0.], rec, [1.]))
    mpre = np.concatenate(([0.], prec, [0.]))

    # compute the precision envelope
    for i in range(mpre.size - 1, 0, -1):
        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])

    # to calculate area under PR curve, look for points
    # where X axis (recall) changes value
    i = np.where(mrec[1:] != mrec[:-1])[0]

    # and sum (\Delta recall) * prec
    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])
    return ap
    
def dataset_pr_info(thresh_num, pr_curve, count_face):
    _pr_curve = np.zeros((thresh_num, 2))
    for i in range(thresh_num):
        _pr_curve[i, 0] = pr_curve[i, 1] / pr_curve[i, 0]
        _pr_curve[i, 1] = pr_curve[i, 1] / count_face
    return _pr_curve

def img_pr_info(thresh_num, pred_info, proposal_list, pred_recall):
    pr_info = np.zeros((thresh_num, 2)).astype(&#39;float&#39;)
    for t in range(thresh_num):

        thresh = 1 - (t+1)/thresh_num
        r_index = np.where(pred_info[:, 4] &gt;= thresh)[0]
        if len(r_index) == 0:
            pr_info[t, 0] = 0
            pr_info[t, 1] = 0
        else:
            r_index = r_index[-1]
            p_index = np.where(proposal_list[:r_index+1] == 1)[0]
            pr_info[t, 0] = len(p_index)
            pr_info[t, 1] = pred_recall[r_index]
    return pr_info

def bbox_overlaps(bboxes1,
                  bboxes2,
                  mode=&#39;iou&#39;,
                  eps=1e-6,
                  use_legacy_coordinate=False):
    &quot;&quot;&quot;Calculate the ious between each bbox of bboxes1 and bboxes2.
    Args:
        bboxes1 (ndarray): Shape (n, 4)
        bboxes2 (ndarray): Shape (k, 4)
        mode (str): IOU (intersection over union) or IOF (intersection
            over foreground)
        use_legacy_coordinate (bool): Whether to use coordinate system in
            mmdet v1.x. which means width, height should be
            calculated as &#39;x2 - x1 + 1` and &#39;y2 - y1 + 1&#39; respectively.
            Note when function is used in `VOCDataset`, it should be
            True to align with the official implementation
            `http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCdevkit_18-May-2011.tar`
            Default: False.
    Returns:
        ious (ndarray): Shape (n, k)
    &quot;&quot;&quot;

    assert mode in [&#39;iou&#39;, &#39;iof&#39;]
    if not use_legacy_coordinate:
        extra_length = 0.
    else:
        extra_length = 1.
    bboxes1 = bboxes1.astype(np.float32)
    bboxes2 = bboxes2.astype(np.float32)
    rows = bboxes1.shape[0]
    cols = bboxes2.shape[0]
    ious = np.zeros((rows, cols), dtype=np.float32)
    if rows * cols == 0:
        return ious
    exchange = False
    if bboxes1.shape[0] &gt; bboxes2.shape[0]:
        bboxes1, bboxes2 = bboxes2, bboxes1
        ious = np.zeros((cols, rows), dtype=np.float32)
        exchange = True
    area1 = (bboxes1[:, 2] - bboxes1[:, 0] + extra_length) * (
        bboxes1[:, 3] - bboxes1[:, 1] + extra_length)
    area2 = (bboxes2[:, 2] - bboxes2[:, 0] + extra_length) * (
        bboxes2[:, 3] - bboxes2[:, 1] + extra_length)
    for i in range(bboxes1.shape[0]):
        x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])
        y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])
        x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])
        y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])
        overlap = np.maximum(x_end - x_start + extra_length, 0) * np.maximum(
            y_end - y_start + extra_length, 0)
        if mode == &#39;iou&#39;:
            union = area1[i] + area2 - overlap
        else:
            union = area1[i] if not exchange else area2
        union = np.maximum(union, eps)
        ious[i, :] = overlap / union
    if exchange:
        ious = ious.T
    return ious
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def image_eval(pred, gt, ignore, iou_thresh):
    &quot;&quot;&quot; single image evaluation
    pred: Nx5
    gt: Nx4
    ignore:
    &quot;&quot;&quot;
    _pred = pred.copy()
    _gt = gt.copy()
    pred_recall = np.zeros(_pred.shape[0])
    recall_list = np.zeros(_gt.shape[0])
    proposal_list = np.ones(_pred.shape[0])

    # _pred[:, 2] = _pred[:, 2] + _pred[:, 0]
    # _pred[:, 3] = _pred[:, 3] + _pred[:, 1]
    _gt[:, 2] = _gt[:, 2] + _gt[:, 0]
    _gt[:, 3] = _gt[:, 3] + _gt[:, 1]

    overlaps = bbox_overlaps(_pred[:, :4], _gt)

    for h in range(_pred.shape[0]):

        gt_overlap = overlaps[h]
        max_overlap, max_idx = gt_overlap.max(), gt_overlap.argmax()
        if max_overlap &gt;= iou_thresh:
            if ignore[max_idx] == 0:
                recall_list[max_idx] = -1
                proposal_list[h] = -1
            elif recall_list[max_idx] == 0:
                recall_list[max_idx] = 1

        r_keep_index = np.where(recall_list == 1)[0]
        pred_recall[h] = len(r_keep_index)
    return pred_recall, proposal_list
    
def get_gt_boxes(gt_dir):
    &quot;&quot;&quot; gt dir: (wider_face_val.mat, wider_easy_val.mat, wider_medium_val.mat, wider_hard_val.mat)&quot;&quot;&quot;

    gt_mat = loadmat(os.path.join(gt_dir, &#39;wider_face_val.mat&#39;))
    hard_mat = loadmat(os.path.join(gt_dir, &#39;wider_hard_val.mat&#39;))
    medium_mat = loadmat(os.path.join(gt_dir, &#39;wider_medium_val.mat&#39;))
    easy_mat = loadmat(os.path.join(gt_dir, &#39;wider_easy_val.mat&#39;))

    facebox_list = gt_mat[&#39;face_bbx_list&#39;]
    event_list = gt_mat[&#39;event_list&#39;]
    file_list = gt_mat[&#39;file_list&#39;]

    hard_gt_list = hard_mat[&#39;gt_list&#39;]
    medium_gt_list = medium_mat[&#39;gt_list&#39;]
    easy_gt_list = easy_mat[&#39;gt_list&#39;]

    return facebox_list, event_list, file_list, hard_gt_list, medium_gt_list, easy_gt_list
    
def load_preds(pred_dir=&#39;/Storage/Projects/pyfeat_testing/Data/WIDER_BBOX_IMG2POSE/preds.pkl&#39;):
    with open(pred_dir, &#39;rb&#39;) as fp:
        all_imgs, all_pred_vals = pickle.load(fp)
    boxes = dict()
    for i, img_name in enumerate(all_imgs):
        event_name = [mai for mai in img_name.split(&#39;/&#39;) if &#39;--&#39; in mai][0]
        if event_name not in boxes:
            boxes[event_name] = {}
        
        pred_box = np.array(all_pred_vals[i])
        boxes[event_name][os.path.basename(img_name).rstrip(&#39;.jpg&#39;)] = pred_box
    return boxes

def norm_score(pred):
    &quot;&quot;&quot; norm score
    pred {key: [[x1,y1,x2,y2,s]]}
    &quot;&quot;&quot;

    max_score = 0
    min_score = 1

    for _, k in pred.items():
        for _, v in k.items():
            if len(v.shape) == 0 or len(v) == 0:
                continue
            _min = np.min(v[:, -1])
            _max = np.max(v[:, -1])
            max_score = max(_max, max_score)
            min_score = min(_min, min_score)

    diff = max_score - min_score
    for _, k in pred.items():
        for _, v in k.items():
            if len(v.shape) == 0 or len(v) == 0:
                continue
            v[:, -1] = (v[:, -1] - min_score)/diff
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def print_ap_scores(result_fp):
    iou_thresh=0.5
    pred = load_preds(pred_dir=result_fp) # Where you save the result in the extract_bbox_img2Pose.py script
    norm_score(pred)
    facebox_list, event_list, file_list, hard_gt_list, medium_gt_list, easy_gt_list = get_gt_boxes(gt_dir=&#39;/Storage/Data/wider/wider_face_split/&#39;) # Where ground-truth are stored
    event_num = len(event_list)
    thresh_num = 1000
    settings = [&#39;easy&#39;, &#39;medium&#39;, &#39;hard&#39;]
    setting_gts = [easy_gt_list, medium_gt_list, hard_gt_list]
    aps = []
    for setting_id in range(3):
        # different setting
        gt_list = setting_gts[setting_id]
        count_face = 0
        pr_curve = np.zeros((thresh_num, 2)).astype(&#39;float&#39;)
        # [hard, medium, easy]
        pbar = tqdm(range(event_num))
        for i in pbar:
            pbar.set_description(&#39;Processing {}&#39;.format(settings[setting_id]))
            event_name = str(event_list[i][0][0])
            img_list = file_list[i][0]
            pred_list = pred[event_name]
            sub_gt_list = gt_list[i][0]
            # img_pr_info_list = np.zeros((len(img_list), thresh_num, 2))
            gt_bbx_list = facebox_list[i][0]

            for j in range(len(img_list)):
                pred_info = pred_list[str(img_list[j][0][0])]

                gt_boxes = gt_bbx_list[j][0].astype(&#39;float&#39;)
                keep_index = sub_gt_list[j][0]
                count_face += len(keep_index)

                if len(gt_boxes) == 0 or len(pred_info.shape) == 0 or len(pred_info) == 0:
                    continue
                ignore = np.zeros(gt_boxes.shape[0])
                if len(keep_index) != 0:
                    ignore[keep_index-1] = 1
                pred_recall, proposal_list = image_eval(pred_info, gt_boxes, ignore, iou_thresh)

                _img_pr_info = img_pr_info(thresh_num, pred_info, proposal_list, pred_recall)

                pr_curve += _img_pr_info
        pr_curve = dataset_pr_info(thresh_num, pr_curve, count_face)

        propose = pr_curve[:, 0]
        recall = pr_curve[:, 1]

        ap = voc_ap(recall, propose)
        aps.append(ap)

    print(&quot;==================== Results ====================&quot;)
    print(&quot;Easy   Val AP: {}&quot;.format(aps[0]))
    print(&quot;Medium Val AP: {}&quot;.format(aps[1]))
    print(&quot;Hard   Val AP: {}&quot;.format(aps[2]))
    print(&quot;=================================================&quot;)
    return aps
</pre></div>
</div>
</div>
</div>
<p>We provide the path for wider dataset, the ground true labels, and where the results are been saved.
Dataset and ground truth labels can be downloaded at http://shuoyang1213.me/WIDERFACE/</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>data_dir = &#39;/Storage/Data/wider/&#39;
true_result_dir = &#39;/Storage/Data/wider/wider_face_split/&#39;
save_result_dir = &#39;/Storage/Projects/pyfeat_testing/Data_Eshin/facebox_test/&#39;
all_imgs = glob.glob(data_dir+&#39;WIDER_val/images/**/*.jpg&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="test-of-faceboxes">
<h1>1. Test of FaceBoxes<a class="headerlink" href="#test-of-faceboxes" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>detector = Detector(face_model=&#39;faceboxes&#39;,emotion_model=&#39;resmasknet&#39;, landmark_model=&#39;mobilefacenet&#39;, au_model=&#39;xgb&#39;, device=&#39;cpu&#39;)
all_pred_vals = []

for img in tqdm(all_imgs):
    im1 = Image.open(img)
    face_aus = detector.detect_faces(im1)
    all_pred_vals.append(face_aus[0])

with open(save_result_dir+&#39;FaceBoxes_bench_results.pkl&#39;, &#39;wb&#39;) as fp:
    pickle.dump((all_imgs, all_pred_vals), fp)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 3226/3226 [02:32&lt;00:00, 21.11it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>facebox_normal = print_ap_scores(result_fp=save_result_dir+&#39;FaceBoxes_bench_results.pkl&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Processing easy: 100%|██████████| 61/61 [00:11&lt;00:00,  5.33it/s]
Processing medium: 100%|██████████| 61/61 [00:11&lt;00:00,  5.33it/s]
Processing hard: 100%|██████████| 61/61 [00:11&lt;00:00,  5.31it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==================== Results ====================
Easy   Val AP: 0.5368750176845414
Medium Val AP: 0.34812514764839486
Hard   Val AP: 0.14662014664396028
=================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="test-of-mtcnn">
<h1>2. Test of MTCNN<a class="headerlink" href="#test-of-mtcnn" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>detector = Detector(face_model=&#39;mtcnn&#39;,emotion_model=&#39;resmasknet&#39;, landmark_model=&#39;mobilefacenet&#39;, au_model=&#39;xgb&#39;, device=&#39;cpu&#39;)
all_pred_vals = []

for img in tqdm(all_imgs):
    im1 = Image.open(img)
    face_aus = detector.detect_faces(im1)
    all_pred_vals.append(face_aus[0])

with open(save_result_dir+&#39;MTCNN_bench_results.pkl&#39;, &#39;wb&#39;) as fp:
    pickle.dump((all_imgs, all_pred_vals), fp)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/tiankang/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:135: UserWarning: Using &#39;backbone_name&#39; as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.
  warnings.warn(
/home/tiankang/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
100%|██████████| 3226/3226 [07:13&lt;00:00,  7.44it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>mtcnn_normal = print_ap_scores(result_fp=save_result_dir+&#39;MTCNN_bench_results.pkl&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Processing easy: 100%|██████████| 61/61 [00:16&lt;00:00,  3.77it/s]
Processing medium: 100%|██████████| 61/61 [00:16&lt;00:00,  3.75it/s]
Processing hard: 100%|██████████| 61/61 [00:16&lt;00:00,  3.75it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==================== Results ====================
Easy   Val AP: 0.7248933447919402
Medium Val AP: 0.7175922904388756
Hard   Val AP: 0.47326227608164284
=================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="test-of-retinaface">
<h1>3. Test of RetinaFace<a class="headerlink" href="#test-of-retinaface" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>detector = Detector(face_model=&#39;retinaface&#39;,emotion_model=&#39;resmasknet&#39;, landmark_model=&#39;mobilefacenet&#39;, au_model=&#39;xgb&#39;, device=&#39;cpu&#39;)
all_pred_vals = []

for img in tqdm(all_imgs):
    im1 = Image.open(img)
    face_aus = detector.detect_faces(im1)
    all_pred_vals.append(face_aus[0])

with open(save_result_dir+&#39;RetinaFace_bench_results.pkl&#39;, &#39;wb&#39;) as fp:
    pickle.dump((all_imgs, all_pred_vals), fp)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/tiankang/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:135: UserWarning: Using &#39;backbone_name&#39; as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.
  warnings.warn(
/home/tiankang/anaconda3/envs/py39/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
  6%|▌         | 178/3226 [00:22&lt;05:13,  9.72it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>retinaface_normal = print_ap_scores(result_fp=save_result_dir+&#39;RetinaFace_bench_results.pkl&#39;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="test-of-img2pose-unconstrained">
<h1>4. Test of Img2Pose unconstrained<a class="headerlink" href="#test-of-img2pose-unconstrained" title="Permalink to this headline">#</a></h1>
<section id="img2pose-models-are-heavy-both-in-architecture-and-in-number-of-hyperparameters-we-advise-to-use-different-parameter-combinations-for-different-settings-especially-for-constrained-vs-unconstrained">
<h2>Img2Pose models are heavy both in architecture and in number of hyperparameters. We advise to use different parameter combinations for different settings, especially for constrained vs unconstrained<a class="headerlink" href="#img2pose-models-are-heavy-both-in-architecture-and-in-number-of-hyperparameters-we-advise-to-use-different-parameter-combinations-for-different-settings-especially-for-constrained-vs-unconstrained" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from feat.facepose_detectors.img2pose.img2pose_test import Img2Pose
from torch.utils.data import Dataset, DataLoader
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class GenericImageDataset(Dataset):
    &quot;&quot;&quot;Face Landmarks dataset.&quot;&quot;&quot;

    def __init__(self, file_paths, transform=None):
        &quot;&quot;&quot;
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        &quot;&quot;&quot;
        self.filePaths = file_paths
        self.transform = transform

    def __len__(self):
        return len(self.filePaths)

    def __getitem__(self, idx):
        img = Image.open(self.filePaths[idx])
        if self.transform:
            img = self.transform(img)
        return img
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>imclassifier = Img2Pose(constrained=False, detection_threshold=0.25, rpn_pre_nms_top_n_test=6000, rpn_post_nms_top_n_test=1000)

img_trans = transforms.Compose([
    transforms.ToTensor()
])

dataset = GenericImageDataset(all_imgs, transform=img_trans)
dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)

all_pred_vals = []
for i_batch, sample_batched in enumerate(tqdm(dataloader)):
    preds = imclassifier(sample_batched)
    all_pred_vals.append(preds[0][0]) # Append Face Bounding Box

# Save Result
with open(save_result_dir+&#39;Img2poseuncon_bench_results.pkl&#39;, &#39;wb&#39;) as fp:
    pickle.dump((all_imgs, all_pred_vals), fp)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 3226/3226 [01:48&lt;00:00, 29.82it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>img2pose_uncon_normal = print_ap_scores(result_fp=save_result_dir+&#39;Img2poseuncon_bench_results.pkl&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Processing easy: 100%|██████████| 61/61 [00:19&lt;00:00,  3.18it/s]
Processing medium: 100%|██████████| 61/61 [00:19&lt;00:00,  3.20it/s]
Processing hard: 100%|██████████| 61/61 [00:18&lt;00:00,  3.21it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==================== Results ====================
Easy   Val AP: 0.8563027227316843
Medium Val AP: 0.8136765059086696
Hard   Val AP: 0.5739961257745989
=================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="test-of-img2pose-constrained">
<h1>5. Test of Img2Pose constrained<a class="headerlink" href="#test-of-img2pose-constrained" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>imclassifier = Img2Pose(constrained=True, detection_threshold=0.25, rpn_pre_nms_top_n_test=2000, rpn_post_nms_top_n_test=200)

img_trans = transforms.Compose([
    transforms.ToTensor()
])

dataset = GenericImageDataset(all_imgs, transform=img_trans)
dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)

all_pred_vals = []
for i_batch, sample_batched in enumerate(tqdm(dataloader)):
    preds = imclassifier(sample_batched)
    all_pred_vals.append(preds[0][0]) # Append Face Bounding Box

# Save Result
with open(save_result_dir+&#39;Img2posecon_bench_results.pkl&#39;, &#39;wb&#39;) as fp:
    pickle.dump((all_imgs, all_pred_vals), fp)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████| 3226/3226 [01:13&lt;00:00, 43.72it/s]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>img2pose_con_normal = print_ap_scores(result_fp=save_result_dir+&#39;Img2posecon_bench_results.pkl&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Processing easy: 100%|██████████| 61/61 [00:16&lt;00:00,  3.60it/s]
Processing medium: 100%|██████████| 61/61 [00:17&lt;00:00,  3.56it/s]
Processing hard: 100%|██████████| 61/61 [00:17&lt;00:00,  3.58it/s]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>==================== Results ====================
Easy   Val AP: 0.6470359115773076
Medium Val AP: 0.5878560483278932
Hard   Val AP: 0.32415904798673495
=================================================
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./extra_tutorials"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="08_train_hogs.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">8. Training HOG-based AU detectors</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="10_test_lands.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">10. Benchmarking Landmark models using data</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Eshin Jolly, Jin Hyun Cheong, Tiankang Xie<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>