{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Py-Feat basics\n",
    "\n",
    "*Written by Eshin Jolly*\n",
    "\n",
    "This tutorial goes over the basics of using Py-Feat's API. You can try it out interactively in Google Collab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cosanlab/py-feat/blob/master/notebooks/content/01_basics.ipynb)\n",
    "\n",
    "At a broad level you will be working with 2 main objects in Py-Feat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below and run this only if you're using Google Collab\n",
    "# !pip install -q py-feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detectors\n",
    "A detector is a swiss-army-knife class that \"glues\" together a particular combination of a Face, Landmark, Action Unit, and Emotion detection model into a single object. This allows us to provide a very easy-to-use high-level API, e.g. `detector.detect_image('my_image.jpg')`, which will automatically make use of the correct underlying model to solve the sub-tasks of identifying face locations, getting landmarks, extracting action units, etc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/mobilenet0.25_Final.pth\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/mobilenet_224_model_best_gdconv_external.pth.tar\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/RF_568.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/ResMaskNet_Z_resmasking_dropout1_rot30.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feat.detector.Detector(face_model=retinaface, landmark_model=mobilenet, au_model=rf, emotion_model=resmasknet, facepose_model=pnp)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from feat.detector import Detector\n",
    "\n",
    "# The verbose flag will print out messages indicating whether a pre-trained model\n",
    "# is being downloaded for the first time, or being loaded from disk.\n",
    "# In this case I already have all the models downloaded, but if you run this notebook on\n",
    "# your personal computer or Google Collab, these will be downloaded the first time you\n",
    "# use a Detector.\n",
    "detector = Detector(verbose=True)\n",
    "\n",
    "detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initializing a detector you can easily swap one or more underlying models using the `.change_model` method. You can also disable any models by setting them to `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/onet.pt\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/pnet.pt\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/rnet.pt\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/svm_568.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/hog_pca_all_emotio.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/hog_scalar_aus.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/emoSVM38.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/emo_hog_pca.joblib\n",
      "Using downloaded and verified file: /Users/Esh/Documents/pypackages/py-feat/feat/resources/emo_hog_scalar.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feat.detector.Detector(face_model=mtcnn, landmark_model=None, au_model=svm, emotion_model=svm, facepose_model=pnp)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector.change_model(\n",
    "    face_model=\"MTCNN\", emotion_model=\"svm\", au_model=\"svm\", landmark_model=None\n",
    ")\n",
    "detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll primarily use a detector instance by calling its `.detect_image()` or `.detect_video()` methods. Both methods take as input a filename (or list of filenames) and return a `Fex` data class instance. Detectors also have lower-level detection methods like `.detect_face()` and `.detect_landmarks()` which operate on and return numpy arrays directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fex data classes\n",
    "A Fex data class is just a special type of pandas dataframe then makes it easier to work with the results returned by a detector. Each row contains information about a detected face and each column contains the output of that detection (e.g. x, y location; emotion) or some meta-data about the input file (e.g. the filename). \n",
    "\n",
    "Fex data instances have helper methods on them to quickly retrieve the appropriate data you want without having to search through column names yourself, e.g.  `fex.emotions()`. They also have methods for plotting e.g. `fex.plot_detections()`, signal-processing e.g. `fex.downsample()`, and statistical analysis e.g. `fex.regress()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from feat.data import Fex\n",
    "import pandas as pd\n",
    "\n",
    "fex = Fex()\n",
    "\n",
    "isinstance(fex, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More often than not, you'll be using `Fex` data classes that are return by `Detector` rather than creating them from scratch. Check out the next tutorial for more details on how this works!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7dcf886e642ffd7132d8e9a6cd5ca71978ba2253d781a5b1b4944468a6c69f78"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py-feat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
