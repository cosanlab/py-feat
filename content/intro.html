
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Py-Feat: Python Facial Expression Analysis Toolbox &#8212; Py-Feat</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Installation example" href="installation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/pyfeat_logo_small.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Py-Feat</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1 current">
  <a class="reference internal" href="#">
   Py-Feat: Python Facial Expression Analysis Toolbox
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Installation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="installation.html">
   Installation example
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="detector.html">
   Detecting FEX from images
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="analysis.html">
   Preprocessing FEX data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="plotting.html">
   Plotting examples
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="loadingOtherFiles.html">
   Loading data from other detectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="train_hogs.html">
   Training HOG-based AU detectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="trainAUvisModel.html">
   Training AU visualization model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="extract_labels_and_landmarks.html">
   Extracting labels and landmarks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="modelContribution.html">
   Contributing new detectors
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  API
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../api/index.html">
   API Reference
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="contribute.html">
   Contribute
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/cosanlab/py-feat">
   GitHub Repository
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/content/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/cosanlab/py-feat"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/cosanlab/py-feat/issues/new?title=Issue%20on%20page%20%2Fcontent/intro.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-you-should-use-py-feat">
   Why you should use Py-Feat
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#who-is-it-for">
   Who is it for?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#installation">
   Installation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-installation">
   Check installation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#available-models">
   Available models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#action-unit-detection">
     Action Unit detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#emotion-detection">
     Emotion detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#face-detection">
     Face detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#facial-landmark-detection">
     Facial landmark detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#face-head-pose-estimation">
     Face/Head pose estimation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contributions">
   Contributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#license">
   License
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Py-Feat: Python Facial Expression Analysis Toolbox</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-you-should-use-py-feat">
   Why you should use Py-Feat
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#who-is-it-for">
   Who is it for?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#installation">
   Installation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#check-installation">
   Check installation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#available-models">
   Available models
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#action-unit-detection">
     Action Unit detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#emotion-detection">
     Emotion detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#face-detection">
     Face detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#facial-landmark-detection">
     Facial landmark detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#face-head-pose-estimation">
     Face/Head pose estimation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#contributions">
   Contributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#license">
   License
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="py-feat-python-facial-expression-analysis-toolbox">
<h1>Py-Feat: Python Facial Expression Analysis Toolbox<a class="headerlink" href="#py-feat-python-facial-expression-analysis-toolbox" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://pypi.org/project/py-feat/"><img alt="Package versioning" src="https://img.shields.io/pypi/v/py-feat.svg" /></a>
<a class="reference external" href="https://travis-ci.org/cosanlab/py-feat/"><img alt="Build Status" src="https://api.travis-ci.org/cosanlab/py-feat.svg?branch=master" /></a>
<a class="reference external" href="https://coveralls.io/github/cosanlab/py-feat?branch=master"><img alt="Coverage Status" src="https://coveralls.io/repos/github/cosanlab/py-feat/badge.svg?branch=master" /></a>
<img alt="Python Versions" src="https://img.shields.io/badge/python-3.6%20%7C%203.7%20%7C%203.8%20%7C%203.9-blue" />
<a class="reference external" href="https://github.com/cosanlab/py-feat/network"><img alt="GitHub forks" src="https://img.shields.io/github/forks/cosanlab/py-feat" /></a>
<a class="reference external" href="https://github.com/cosanlab/py-feat/stargazers"><img alt="GitHub stars" src="https://img.shields.io/github/stars/cosanlab/py-feat" /></a>
<a class="reference external" href="https://zenodo.org/badge/latestdoi/118517740"><img alt="DOI" src="https://zenodo.org/badge/118517740.svg" /></a></p>
<p>Py-Feat provides a comprehensive set of tools and models to easily detect facial expressions (Action Units, emotions, facial landmarks) from images and videos, preprocess &amp; analyze facial expression data, and visualize facial expression data.</p>
<div class="section" id="why-you-should-use-py-feat">
<h2>Why you should use Py-Feat<a class="headerlink" href="#why-you-should-use-py-feat" title="Permalink to this headline">¶</a></h2>
<p>Facial expressions convey rich information about how a person is thinking, feeling, and what they are planning to do. Recent innovations in computer vision algorithms and deep learning algorithms have led to a flurry of models that can be used to extract facial landmarks, Action Units, and emotional facial expressions with great speed and accuracy. However, researchers seeking to use these algorithms or tools such as <a class="reference external" href="https://github.com/TadasBaltrusaitis/OpenFace">OpenFace</a>, <a class="reference external" href="https://imotions.com/">iMotions</a>-<a class="reference external" href="https://www.affectiva.com/science-resource/affdex-sdk-a-cross-platform-realtime-multi-face-expression-recognition-toolkit/">Affectiva</a>, or <a class="reference external" href="https://www.noldus.com/facereader/">Noldus FaceReacer</a> may find them difficult to install, use, or too expensive to purchase. It’s also difficult to use the latest model or know exactly how good the models are for proprietary tools. <strong>We developed Py-Feat to create a free, open-source, and easy to use tool for working with facial expressions data.</strong></p>
</div>
<div class="section" id="who-is-it-for">
<h2>Who is it for?<a class="headerlink" href="#who-is-it-for" title="Permalink to this headline">¶</a></h2>
<p>Py-Feat was created for two primary audiences in mind:</p>
<ul class="simple">
<li><p><strong>Human behavior researchers</strong>: Extract facial expressions from face images or videos with a simple line of code and analyze your data with Feat.</p></li>
<li><p><strong>Computer vision researchers</strong>: Develop &amp; share your latest model to a wide audience of users.</p></li>
</ul>
<p>and anyone else interested in analyzing facial expressions!</p>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>Install from <a class="reference external" href="https://pypi.org/project/py-feat/">pip</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">py</span><span class="o">-</span><span class="n">feat</span>
</pre></div>
</div>
<p>Install from <a class="reference external" href="https://github.com/cosanlab/feat">source</a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">cosanlab</span><span class="o">/</span><span class="n">feat</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">feat</span> <span class="o">&amp;&amp;</span> <span class="n">python</span> <span class="n">setup</span><span class="o">.</span><span class="n">py</span> <span class="n">install</span>
</pre></div>
</div>
<p>You can install it in <a class="reference external" href="http://colab.research.google.com/">Google Colab</a> or <a class="reference external" href="http://kaggle.com/">Kaggle</a> using the code above. You can also install it in Development Mode:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>!git clone https://github.com/cosanlab/feat.git  
!cd feat &amp;&amp; pip install -q -r requirements.txt
!cd feat &amp;&amp; pip install -q -e . 
!cd feat &amp;&amp; python bin/download_models.py
# Click Runtime from top menu and Restart Runtime! 
</pre></div>
</div>
<p>The last development mode installation using the <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-e</span> <span class="pre">.</span></code> can also be useful when contributing to Py-Feat.</p>
</div>
<div class="section" id="check-installation">
<h2>Check installation<a class="headerlink" href="#check-installation" title="Permalink to this headline">¶</a></h2>
<p>Import the Fex class</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">feat</span> <span class="kn">import</span> <span class="n">Fex</span>
</pre></div>
</div>
<p>Import the Detector class</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">feat</span> <span class="kn">import</span> <span class="n">Detector</span>
</pre></div>
</div>
</div>
<div class="section" id="available-models">
<h2>Available models<a class="headerlink" href="#available-models" title="Permalink to this headline">¶</a></h2>
<p>Below is a list of models implemented in Py-Feat and ready to use. The model names are in the titles followed by the reference publications.</p>
<div class="section" id="action-unit-detection">
<h3>Action Unit detection<a class="headerlink" href="#action-unit-detection" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">rf</span></code>: Random Forest model trained on Histogram of Oriented Gradients extracted from BP4D, DISFA, CK+, UNBC-McMaster shoulder pain, and AFF-Wild2 datasets</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">svm</span></code>: SVM model trained on Histogram of Oriented Gradients extracted from BP4D, DISFA, CK+, UNBC-McMaster shoulder pain, and AFF-Wild2 datasets</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logistic</span></code>: Logistic Classifier model trained on Histogram of Oriented Gradients extracted from BP4D, DISFA, CK+, UNBC-McMaster shoulder pain, and AFF-Wild2 datasets</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">JAANET</span></code>: Joint facial action unit detection and face alignment via adaptive attention trained with BP4D and BP4D+ (<a class="reference external" href="https://arxiv.org/pdf/2003.08834v1.pdf">Shao et al., 2020</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DRML</span></code>: Deep region and multi-label learning for facial action unit detection by (<a class="reference external" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhao_Deep_Region_and_CVPR_2016_paper.pdf">Zhao et al., 2016</a>)</p></li>
</ul>
</div>
<div class="section" id="emotion-detection">
<h3>Emotion detection<a class="headerlink" href="#emotion-detection" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">rf</span></code>: Random Forest model trained on Histogram of Oriented Gradients extracted from ExpW, CK+, and JAFFE datasets</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">svm</span></code>: SVM model trained on Histogram of Oriented Gradients extracted from ExpW, CK+, and JAFFE datasets</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fernet</span></code>: Deep convolutional network</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ResMaskNet</span></code>: Facial expression recognition using residual masking network by (<a class="reference external" href="https://ailb-web.ing.unimore.it/icpr/author/3818">Pham et al., 2020</a>)</p></li>
</ul>
</div>
<div class="section" id="face-detection">
<h3>Face detection<a class="headerlink" href="#face-detection" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">MTCNN</span></code>: Multi-task cascaded convolutional networks by (<a class="reference external" href="https://arxiv.org/pdf/1604.02878.pdf">Zhang et al., 2016</a>; <a class="reference external" href="https://ieeexplore.ieee.org/document/9239720">Zhang et al., 2020</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FaceBoxes</span></code>: A CPU real-time fae detector with high accuracy by (<a class="reference external" href="https://arxiv.org/pdf/1708.05234v4.pdf">Zhang et al., 2018</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">RetinaFace</span></code>: Single-stage dense face localisation in the wild by (<a class="reference external" href="https://arxiv.org/pdf/1905.00641v2.pdf">Deng et al., 2019</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">img2pose</span></code>: Face Alignment and Detection via 6DoF, Face Pose Estimation (<a class="reference external" href="https://arxiv.org/pdf/2012.07791v2.pdf">Albiero et al., 2020</a>). Performs simultaneous (one-shot) face detection and head pose estimation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">img2pose-c</span></code>: A ‘constrained’ version of the above model, fine-tuned on images of frontal faces with pitch, roll, yaw measures in the range of (-90, 90) degrees. Shows lesser performance on difficult face detection tasks, but state-of-the-art performance on face pose estimation for frontal faces</p></li>
</ul>
</div>
<div class="section" id="facial-landmark-detection">
<h3>Facial landmark detection<a class="headerlink" href="#facial-landmark-detection" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">PFLD</span></code>: Practical Facial Landmark Detector by (<a class="reference external" href="https://arxiv.org/pdf/1902.10859.pdf">Guo et al, 2019</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MobileFaceNet</span></code>: Efficient CNNs for accurate real time face verification on mobile devices (<a class="reference external" href="https://arxiv.org/ftp/arxiv/papers/1804/1804.07573.pdf">Chen et al, 2018</a>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MobileNet</span></code>: Efficient convolutional neural networks for mobile vision applications (<a class="reference external" href="https://arxiv.org/pdf/1704.04861v1.pdf">Howard et al, 2017</a>)</p></li>
</ul>
</div>
<div class="section" id="face-head-pose-estimation">
<h3>Face/Head pose estimation<a class="headerlink" href="#face-head-pose-estimation" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">img2pose</span></code>: Face Alignment and Detection via 6DoF, Face Pose Estimation (<a class="reference external" href="https://arxiv.org/pdf/2012.07791v2.pdf">Albiero et al., 2020</a>). Performs simultaneous (one-shot) face detection and head pose estimation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">img2pose-c</span></code>: A ‘constrained’ version of the above model, fine-tuned on images of frontal faces with pitch, roll, yaw measures in the range of (-90, 90) degrees. Shows lesser performance on hard face detection tasks, but state-of-the-art performance on head pose estimation for frontal faces.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Perspective-n-Point</span></code>: <a class="reference external" href="https://link.springer.com/article/10.1007/s11263-008-0152-6">Efficient PnP (EPnP)</a> method implemented via <code class="docutils literal notranslate"><span class="pre">cv2</span></code> to solve the <a class="reference external" href="https://en.wikipedia.org/wiki/Perspective-n-Point">Perspective n Point</a> (PnP) problem to obtain 3D head pose from 2D facial landmarks</p></li>
</ul>
</div>
</div>
<div class="section" id="contributions">
<h2>Contributions<a class="headerlink" href="#contributions" title="Permalink to this headline">¶</a></h2>
<p>We are excited for people to add new models and features to Py-Feat. Please see the <a class="reference external" href="https://cosanlab.github.io/feat/content/contribute.html">contribution guides</a>.</p>
</div>
<div class="section" id="license">
<h2>License<a class="headerlink" href="#license" title="Permalink to this headline">¶</a></h2>
<p>Py-FEAT is provided under the  <a class="reference external" href="https://github.com/cosanlab/py-feat/blob/master/LICENSE">MIT license</a>. You also need to cite and respect the licenses of each model you are using. Please see the LICENSE file for links to each model’s license information.</p>
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='right-next' id="next-link" href="installation.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Installation example</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Jin Hyun Cheong, Tiankang Xie, Eshin Jolly<br/>
    
        &copy; Copyright 2022.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>